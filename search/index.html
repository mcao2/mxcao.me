<!DOCTYPE html>
<html lang="en-us">

  <head>
  <meta charset="utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- Meta -->
  <link rel="canonical" href="https://mxcao.me/">

  <!-- Analytics -->
  <script async src="https://umami.mxcao.me/script.js" data-website-id="4c2d93d9-a90e-41c6-bc74-d2f96994b1e9"></script>

  <!-- Christmax Snow -->
  <!-- <script src="https://app.embed.im/snow.js" defer></script> -->

  <title>
     &middot; mxin
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/main.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet"
    href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700%7CSource+Sans+Pro:400,400i,700,700i&subset=greek,greek-ext,latin-ext">

  <!-- Icons -->
  <link rel="apple-touch-icon" sizes="180x180" href="/public/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/public/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/public/favicon-16x16.png">
  <link rel="manifest" href="/public/site.webmanifest">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

</head>

  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>mxin</h1>
      <p class="tagline"># Mengxin Cao</p>
    </div>

    <input type="checkbox" id="menu-icon">
    <label class="menu-label" for="menu-icon"></label>
    <div class="trigger">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-item">
          <a href="/">Home</a>
        </li>
        

        
        
        
        
        
        
        
        
        <li class="sidebar-nav-item">
          <a href="/about/">About</a>
        </li>
        
        
        
        
        
        
        
        
        
        
        
        <li class="sidebar-nav-item">
          <a href="/posts/">Posts</a>
        </li>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <li class="sidebar-nav-item">
          <a href="https://github.com/mcao2/">GitHub</a>
        </li>
        <li class="sidebar-nav-item">
          <a href="https://www.linkedin.com/in/mxin/">LinkedIn</a>
        </li>
        <!-- <li class="sidebar-nav-item">
          <a href="https://cs.cmu.edu/~mcao2/mengxin_cv.pdf">Curriculum Vitae</a>
        </li> -->
      </ul>

      <p class="copyright">&copy; 2023. All rights reserved.</p>
    </div>
  </div>
</div>

    <div class="content container">
      <form action="/search/" method="get">
  <label for="search-box">Search</label>
  <input type="text" id="search-box" name="query" />
  <input type="submit" value="search" />
</form>

<ul id="search-results"></ul>

<script>
  window.store = {
    
      "2023-06-22-how-to-setup-ptr-record-in-oci": {
        "title": "How to setup PTR record in Oracle Cloud Infrastructure (OCI)",
        "content": "IntroductionOne of my VPS machines use Cloudron to easily self-host apps. And I always receive a notification from my Cloudron instance saying:PTR DNS record (PTR) did not match.    Hostname: &lt;IP&gt;    Expected: &lt;My Domain Name&gt;    Actual: null  PTR: A Pointer record reverse maps an IP address to a hostname. This behavior is the opposite of an A Record, which forward maps a hostname to an IP address. PTR records are commonly found in reverse DNS zones. For more information about PTR records, see RFC 1035.1So I looked it up and it takes me to OCI’s Reverse DNS. As it says, reverse DNS maps an IP to a hostname.Even though my DNS records are managed by Cloudflare, the IP belongs to OCI. PTR records are primarily used for reverse DNS lookup, and their management is typically handled by the owner of the IP address range, which, in this case, is OCI.How-to2      Create an A (IPv4) or AAAA (IPv6) forward record that points your FQDN to the IP.        Open a service ticket for OCI and include the IP and FQDN that you want in the PTR. Note that you need to explicitly mention that you have finished step 1 in the ticket.  VerificationAfter OCI team added the PTR record, we can verify by executing dig -x &lt;IP&gt; and check the ANSWER SECTION contains the PTR record.E.g. if your IP is 1.2.3.4 and the mapped FQDN is example.com, then dig -x 1.2.3.4 would return the following in the ANSWER SECTION:4.3.2.1.in-addr.arpa. 3600 IN\tPTR\texample.com.References            https://docs.oracle.com/en-us/iaas/Content/DNS/Reference/supporteddnsresource.htm#types__dlentry_ptr &#8617;              https://docs.oracle.com/en-us/iaas/Content/Network/Concepts/reverse_dns.htm &#8617;      ",
        "url": "/2023/06/22/how-to-setup-ptr-record-in-oci/"
      }
      ,
    
      "2023-06-21-debug-resource-deadlock-avoided": {
        "title": "Debug Resource Deadlock Avoided Error",
        "content": "IntroductionWhen I was debugging a core dump issue discovered in ADS (Autonomous Driving System) testing, I found a very interesting error message in the log file:terminate called after throwing an instance of 'boost::interprocess::interprocess_exception'  what():  Resource deadlock avoidedAt first I thought there might be some bugs in my code when using the interprocess filelock in boost library. But after some investigation, I found that this error message is actually from the system.The scenario is that we have multiple processes and each process may have multiple threads concurrently accessing our map data. The data is organized into multiple files and each file is protected by a filelock. The filelock is implemented using the interprocess filelock in boost library and each thread can grab a RLock or WLock to access the file. The RLock is shared among threads and the WLock is exclusive to the thread that grabs it.Root causeThe root cause of this issue is that the operating system doesn’t have a deadlock detection granularity at the thread level, only at the process level.What this means is that if we have 2 processes, each process have 2 threads (name them P1T1, P1T2, P2T1 and P2T2), then:  at timestamp t1, P1T1 grabs a write lock on file D1 and P2T1 grabs a write lock on file D2  at timestamp t2, P1T2 attempts to grab a read lock on file D2 and P2T2 attempts to grab a read lock on file D1Although there is no real deadlocks at timestamp t2, the OS thinks there is a deadlock because it sees P1 is waiting for P2 and P2 is waiting for P1.VerificationBelow I provide a simple C++ program to reproduce this issue. The program has 2 processes, each process has 2 threads. Each thread grabs a lock on a file and sleeps for a while. The first process grabs a write lock on file D1 and a read lock on file D2. The second process grabs a write lock on file D2 and a read lock on file D1.Note that some internal implementation details are omitted for simplicity.If we run the program, we can reproduce the Resource deadlock avoided error message.#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;#include &lt;thread&gt;#include \"flock.h\" // internal filelock implementation#include \"util.h\"void routine_read_block(const std::string&amp; block_name) {  std::string lock_name = fmt::format(\"{}.lock\", block_name);  FileLock lock(lock_name);  lock.Readlock();  SPDLOG_INFO(\"read lock on {} acquired\", lock_name);  sleep(3);  lock.Unlock();  SPDLOG_INFO(\"read lock on {} released\", lock_name);}void routine_write_block(const std::string&amp; block_name) {  std::string lock_name = fmt::format(\"{}.lock\", block_name);  FileLock lock(lock_name);  lock.Writelock();  SPDLOG_INFO(\"write lock on {} acquired\", lock_name);  sleep(5);  lock.Unlock();  SPDLOG_INFO(\"write lock on {} released\", lock_name);}int main() {  SPDLOG_INFO(\"=========2 process, each 2 threads=========\");  {    auto pid1 = fork();    if (pid1 == 0) {      // child process      std::vector&lt;std::thread&gt; threads;      threads.emplace_back(routine_write_block, \"D1\");      sleep(1);      threads.emplace_back(routine_read_block, \"D2\");      for (auto&amp; t : threads) {        t.join();      }    } else {      // parent process      std::vector&lt;std::thread&gt; threads;      threads.emplace_back(routine_write_block, \"D2\");      sleep(1);      threads.emplace_back(routine_read_block, \"D1\");      for (auto&amp; t : threads) {        t.join();      }      wait(NULL);    }  }  return 0;}SolutionThe solution is to retry the lock acquisition a limited number of times when the Resource deadlock avoided error message is thrown and random sleep for a while before retrying.References  https://gist.github.com/harrah/4714661",
        "url": "/2023/06/21/Debug-resource-deadlock-avoided/"
      }
      ,
    
      "tools-2022-08-28-setup-vmess-edge": {
        "title": "Set up network edge router via V2Ray",
        "content": "MotivationHello! Long time no see! I have been heavily using an iOS app called “Quantumult X” (a.k.a. QX) these days, mainly for the following reasons:  Easy to setup and lots of tutorials/scripts online that you can add and learn  You can add VPN servers, add flexible routing rules e.g., SSID-based switching, and subscribe to public servers (not recommended out of privacy/security concerns)  You can do MITM rewrites that enable you to have premium features on other apps for freeOverall I’m very happy with this app but I do find some limitations:  The set of protocols supported by QX is limited. According to their sample.conf, it only supports shadowsocks, vmess, http, and trojan.For me I do have some proxy servers using shadowsocks, vmess, but I also have other types up and running such as wireguard. So natively there’s no way you can add a wireguard server to this list of server_local in QX.Besides that I have other good-to-have features that I wish I can use within a single app:  iOS only supports 1 VPN up and running and therefore if I connect to QX then my tailscale connection is down. This is okay but it would be great if I can access my services deployed within the tailscale subnet.  Even if this single app QX is capable to handle all the protocols, it is still limited to my mobile devices and I wish I can have some sort of routing done on the server side rather than on this app, so that if I switch to another app or a different platform then I can still have the same set of routing rules and servers. This is what I call the “network edge router” which is simply a VPS server that helps me route my traffic to different VPN servers.          The added benefit of this approach is that the VPS server is much more performant and flexible since it is a Linux machine so it can do much complicated things like multi-hop connections (e.g., device A -&gt; Network Edge Router -&gt; VPN@Server1 -&gt; VPN@Server2), which will greatly improve your anonymity.      In this post I will share how I set up such a network edge router with the help of V2Ray. Specifically I’m deploying this service to my OCI Ampere machine (arm64). Check out my previous post about setting up an OCI instance if you are interested.Before we come to the v2ray server config itself, let’s add a few VPN servers that we will use later.Set up a Cloudfare Warp+ VPN (or any wireguard server)One thing great about Cloudfare Warp+ is that you can convert it to a wireguard server with the help of wgcf.The conversion steps are simple and straightforward, just follow their official README and you should be good to go. Below is my steps for my future referencewget https://github.com/ViRb3/wgcf/releases/download/v2.2.15/wgcf_2.2.15_linux_arm64ln -s wgcf_2.2.15_linux_arm64 wgcf./wgcf register -n '&lt;machine name&gt;' --accept-tosWGCF_LICENSE_KEY=\"&lt;your license key&gt;\" ./wgcf update./wgcf generateAfter the above steps you should have a wireguard conf file named wgcf-profile.conf by default.Use wireguard as a socks5 serverWe don’t want to directly use the generated wireguard config because it will route all our VPS traffic through Cloudfare. What we want is an application-based proxy. So here I will convert it into a socks5 proxy server by using a docker container. I know this is not the most efficient approach but this is simpler and easy to follow.First check out the content of the generated wireguard conf file in the previous step and remove all the IPv6 contents within this file e.g., your wireguard IPv6 address, and the IPv6 CIDR in the AllowedIPs. This step is necesssary because our wireguard-socks5 docker container cannot process such rules.Clone this repo and build your own wireguard-socks5 image. Note that you may want to change the network interface here used in the container if you encounter any issues (e.g., change it to eth0). Note that after the change you should rebuild the docker image.Below is my steps:git clone git@github.com:mcao2/wireguard-socks5.gitpodman build -t wireguard-socks5:latest-arm .# First run an interactive container to check if there's any errorspodman run --rm -it \\  --name=wireguard-socks-proxy \\  --device=/dev/net/tun --cap-add=NET_ADMIN --privileged \\  --publish 127.0.0.1:1080:1080 \\  --volume /my/dir/to/wireguard:/etc/wireguard:z \\  wireguard-socks5:latest-arm# If you encounter network interface name resolution error then change it in https://github.com/mcao2/wireguard-socks5/blob/master/sockd.conf and rebuild the docker# Now start our proxy server in detach modepodman run --rm -d \\  --name=wireguard-socks-proxy \\  --device=/dev/net/tun --cap-add=NET_ADMIN --privileged \\  --publish 127.0.0.1:1080:1080 \\  --volume /my/dir/to/wireguard:/etc/wireguard:z \\  wireguard-socks5:latest-armBy now you should have a socks5 server up and running in 127.0.0.1:1080, verify this by using curl --proxy socks5h://127.0.0.1:1080 ipinfo.io.Auto start container on restartPodman provides command to generate a systemd unit file that you can enable for this purpose. Below is my steps to enable this:sudo susetsebool -P container_manage_cgroup on# `--name` is the container namepodman generate systemd --files --name wireguard-socks-proxy --newmv container-wireguard-socks-proxy.service /etc/systemd/system/container-wireguard-socks-proxy.servicesystemctl enable container-wireguard-socks-proxy.service# Stop your existing container firstpodman rm -f wireguard-socks-proxy# Start your new container via systemdsystemctl start container-wireguard-socks-proxy.service# Check its staticsystemctl status container-wireguard-socks-proxy.serviceV2Ray rulesNow is the fun part! Install the latest V2Ray service following their guides.Edit the config file /usr/local/etc/v2ray/config.json and add the following contents:{  \"log\": {    \"loglevel\": \"warning\",    \"access\": \"/var/log/v2ray/access.log\",    \"error\": \"/var/log/v2ray/error.log\"  },  \"inbounds\": [    {      \"tag\": \"vmess-in\",      // CHANGE ME!      \"port\": &lt;YOUR_PORT_1&gt;,      \"listen\": \"0.0.0.0\",      \"protocol\": \"vmess\",      \"settings\": {        \"clients\": [ // An array for valid user accounts          {            // CHANGE ME!            \"id\": \"&lt;YOUR_UUID_1&gt;\", // User ID, in the form of a UUID            \"alterId\": 64, // Number of alternative IDs, which will be generated in a deterministic way            \"level\": 0 // V2Ray will apply different policies based on user level          }        ],        \"disableInsecureEncryption\": true // Forbids client for using insecure encryption methods      }    },    {      \"tag\": \"telegram-in\",      // CHANGE ME!      \"port\": &lt;YOUR_PORT_2&gt;,      \"listen\": \"0.0.0.0\",      \"protocol\": \"mtproto\",      \"settings\": {        \"users\": [          {            \"level\": 0,            // CHANGE ME!            \"secret\": \"&lt;YOUR_SECRET_2&gt;\" // User secret. In Telegram, user secret must be 32 characters long, and only contains characters between 0 to 9, and ato f. You may use the following command to generate MTProto secret: `openssl rand -hex 16`          }        ]      }    },    {      \"tag\": \"vmess-in-cloudfare\",      // CHANGE ME!      \"port\": &lt;YOUR_PORT_3&gt;,      \"listen\": \"0.0.0.0\",      \"protocol\": \"vmess\",      \"settings\": {        \"clients\": [          {            // CHANGE ME!            \"id\": \"&lt;YOUR_UUID_3&gt;\",            \"alterId\": 64,            \"level\": 0          }        ],        \"disableInsecureEncryption\": true      }    }  ],  \"outbounds\": [    {      \"tag\": \"default-out\",      \"protocol\": \"freedom\",      \"settings\": {}    },    {      \"tag\": \"telegram-out\",      \"protocol\": \"mtproto\",      \"settings\": {}    },    {      \"tag\": \"tailscale-out\",      \"protocol\": \"freedom\",      \"settings\": {}    },    {      \"tag\": \"cloudfare-out\",      \"protocol\": \"socks\",      \"settings\": {        \"servers\": [          {            \"address\": \"127.0.0.1\",            \"port\": 1080          }        ]      }    }  ],  \"routing\": { // Configuration for internal Routing strategy    \"domainStrategy\": \"AsIs\", // domain resolution strategy    \"rules\": [ // for each inbound connection, v2ray tries these rules from top down one by one. If a rule takes effect, the connection will be routed to the `outboundTag` or `balanceTag` of the rule      { // Route traffic for the `mtproto` protocol        \"type\": \"field\",        \"inboundTag\": [          \"telegram-in\"        ],        \"outboundTag\": \"telegram-out\"      },      { // Route traffic for tailscale        \"type\": \"field\",        \"ip\": [          \"100.64.0.0/10\" // tailscale subnet        ],        \"outboundTag\": \"tailscale-out\"      },      { // Route vmess-in via default out        \"type\": \"field\",        \"inboundTag\": [          \"vmess-in\"        ],        \"outboundTag\": \"default-out\"      },      { // Route vmess-in-cloudfare via cloudfare wireguard interface        \"type\": \"field\",        \"inboundTag\": [          \"vmess-in-cloudfare\"        ],        \"outboundTag\": \"cloudfare-out\"      }    ]  }}The config file is self-explanatory and you definitely need to change all the fields marked with // CHANGE ME!.In this config file I also added a mtproto server for my telegram client to use. You can choose not to add this andsafely remove all associated routing rules.That’s it! In your QX config add the following:vmess=&lt;YOUR_VPS_IP&gt;:&lt;YOUR_PORT_1&gt;, method=chacha20-poly1305, password=&lt;YOUR_UUID_1&gt;, fast-open=false, udp-relay=false, tag=vmess-amperevmess=&lt;YOUR_VPS_IP&gt;:&lt;YOUR_PORT_3&gt;, method=chacha20-poly1305, password=&lt;YOUR_UUID_3&gt;, fast-open=false, udp-relay=false, tag=vmess-ampere-cloudfareEnjoy!",
        "url": "/tools/2022/08/28/setup-vmess-edge/"
      }
      ,
    
      "tools-2021-05-28-setup-mikrotik-ike2": {
        "title": "Set up IKEv2 VPN on a Mikrotik Router",
        "content": "MotivationI got rid of my AT&amp;T router a few days back and managed to have a Mikrotik router hAP ac² as an alternative. One benefit of using a mikrotik router, specifically, its RouterOS, is the customizability to add cool features to my home network. I have very slow traffic when using the AT&amp;T home network when visiting some websites, e.g., weibo, so I want to route them via a VPN running on oracle cloud.Check out my previous post about setting up an OCI instance if you are interested.Set up an IPSec VPNI usually use Tailscale for VPNs but seems mikrotik does not have it for now. So I found an IPSec VPN auto setup script to ease the burden. Please checkout this repo for a detailed walkthrough.For Ubuntu &amp; Debian, the one-liner script is:  wget https://git.io/vpnsetup -O vpn.sh &amp;&amp; sudo sh vpn.sh &amp;&amp; sudo ikev2.sh --autoSet up an IKEv2 client on the Mikrotik routerYou can find some tutorials on setting up a NordVPN on a RouterOS, like this one and most of the steps are similar to what we need to do.Step 0: Import your .p12 fileThis .p12 file acts like the all-in-one cert and is usually encrypted with a passphrase. You can find it in the output of the previous step when you setting up the VPN server.Upload your .p12 file to the Files of the mikrotik router. You can verify the file is accessible by using the following command:  /file printAnd you will find something like this:  # NAME                                       TYPE                                            SIZE CREATION-TIME  0 vpnclient.p12                              .p12 file                                       4425 may/27/2021 22:00:00Next import this .p12 file via:  /certificate import file-name=vpnclient.p12 passphrase=YOUR_PASSPHRASERemember to substitute your passphrase in the above command. And you probably need to run this command twice until you see the number of private-keys-imported is 1.Then go to the certificates tab of the web portal, check the one with KT marker, note it down and it will be the one we will use in the next steps.Step 1: Set up the IKEv2 client 1Create a separate Phase 1 profile and Phase 2 proposal configurations to not interfere with any existing IPsec configuration:  /ip ipsec profile  add name=ike2-oracle-vpn  /ip ipsec proposal  add name=ike2-oracle-vpn pfs-group=noneCreate a new policy group and template to separate this configuration from any other IPsec configuration:  /ip ipsec policy group  add name=ike2-oracle-vpn  /ip ipsec policy  add group=ike2-oracle-vpn proposal=ike2-oracle-vpn template=yes dst-address=0.0.0.0/0 src-address=0.0.0.0/0Create a new mode config entry with responder=no that will request configuration parameters from the server:  /ip ipsec mode-config  add name=ike2-oracle-vpn responder=noCreate peer and identity configurations, remember to substitute yoru server IP and the correct certificate filename in the following command:  /ip ipsec peer  add address=123.123.123.123/32 exchange-mode=ike2 name=ike2-oracle-vpn profile=ike2-oracle-vpn  /ip ipsec identity  # vpnclient.p12_1 is the one with `KT` marker on it  add auth-method=digital-signature certificate=vpnclient.p12_1 generate-policy=port-strict mode-config=ike2-oracle-vpn peer=ike2-oracle-vpn policy-template-group=ike2-oracle-vpnVerify that the connection is successfully established:  /ip ipsec  active-peers print  installed-sa printYou should see an established connection to your VPN server.Step 2: Prepare the list of IPs to be sent over the tunnelGrab some existing IP-list and wrangle it to fit RouterOS. The following command will get a list of China IP ranges 2, add them to a list named CNIP, and prepare them so that they can be easily imported to the mikrotik router.  curl -s https://raw.githubusercontent.com/17mon/china_ip_list/master/china_ip_list.txt |sed -e 's/^/add address=/g' -e 's/$/ list=CNIP/g'|sed -e $'1i\\\\\\n/ip firewall address-list' -e $'1i\\\\\\nremove [/ip firewall address-list find list=CNIP]' &gt;cnip.rscNow same trick: upload this file to the router, and import it via /import cnip.rsc. Note that this list is huge and it may take a while to ingest it.Step 3: Tag traffic that match the list in Mangle FirewallSet the connection-mark under your mode config configuration:  /ip ipsec mode-config  set [ find name=ike2-oracle-vpn ] connection-mark=ike2-oracle-vpnWhen it is done, a NAT rule is generated with the dynamic address provided by the server:  [admin@MikroTik] /ip firewall mangle&gt; /ip firewall nat print  Flags: X - disabled, I - invalid, D - dynamic   0  D ;;; ipsec mode-config        chain=srcnat action=src-nat to-addresses=192.168.43.10 connection-mark=ike2-oracle-vpnApply connection-mark to traffic matching the created address list:  /ip firewall mangle  add action=mark-connection chain=prerouting dst-address-list=CNIP new-connection-mark=ike2-oracle-vpn passthrough=yesReferences            https://wiki.mikrotik.com/wiki/Manual:IP/IPsec#RouterOS_client_configuration &#8617;              https://www.willnet.net/index.php/archives/369/ &#8617;      ",
        "url": "/tools/2021/05/28/setup-mikrotik-ike2/"
      }
      ,
    
      "infrastructure-2021-05-23-notes-on-oci-usage": {
        "title": "Notes on OCI (Oracle Cloud Infrastructure)",
        "content": "MotivationSome of my services are too light that I don’t want to run them endlessly on my beefy home server. One reason is that I want better availability and clearly it’s hard (a better word probably is expensive) to achieve that in my current set up. So I decided to go for the Cloud, after all… I was a cloud TA!I chose OCI because they offer two always-free machines and it seems they are serious about this offer. So I migrated several of my services to OCI instances. This post is a summary of the quirks that I want to note down for future reference.FirewallI don’t know why they make it so hard to achieve the same effect as AWS security groups… But anyway, here’s what I need to do to reach my service via the public Internet.Set up VPC rules through the portalIn the instance detail page, click Attached VNICs under the Resources tab. Then find the subnet of VLAN that this instance is attached with. You will be directed to the subnet detail page, where you can find the Security Lists, and that is our target (the equivalent of AWS security group). Then you can add/remove any rules you want.Set up iptable rules via command lineThis iptable thing actually confuses me a lot. I don’t know why they enforce iptable rules together with the security list set up.SSH into the instance and run the following commands. These commands are taken from this SO post  sudo iptables-save &gt; ~/iptables-rules # backup  sudo iptables -P INPUT ACCEPT # the following commands effectively disable iptables by allowing all traffic  sudo iptables -P OUTPUT ACCEPT  sudo iptables -P FORWARD ACCEPT  sudo iptables -F",
        "url": "/infrastructure/2021/05/23/notes-on-oci-usage/"
      }
      ,
    
      "infrastructure-2020-08-28-private-docker-registry": {
        "title": "Private Docker Registry",
        "content": "MotivationI have been using docker containers for a while and quite amazed by its simplicity and power. Someday in the previous week when I was migrating/re-deploying my pleroma instance from my SurfaceGo ubuntu machine to my home lab, I found I need more private repos on docker hub. Docker hub by default only provides 1 private repo for normal users, which is clearly not what I want. So I decided to build my own docker registry and deploy it as a service in my home lab machine.My set up is:Run a front-facing machine with a public IPv4 address, and add a DNS A record (registry.mxcao.me) to point to this address. The front-facing machine will route all valid traffic to the registry container running in my Rancher cluster.Since my home lab do not have a public IPv4 address dedicated for such purpose, I use a DigitalOcean instance to serve the purpose, along with my pleroma service routing (feel free to play around pleroma.mxcao.me).To be able to route my traffic to the registry container, I use WireGuard to connect these two services. Definitely check out my previous post about setting up WireGuard if you are interested.PrerequisiteCheck you have the following items ready:  Your own domain so that it can be reached from the public Internet  A front-facing machine with          a public IPv4 address (e.g. 1.2.3.4)      nginx      WireGuard        Docker (or Rancher alike) installed on the home lab to host our registry serviceStepsSet up DNS recordI want my registry service accessible from anywhere with a memorizable address and therefore I need to set up a A record to point pleroma.mxcao.me to 1.2.3.4. You can omit this step if you can accept the inconvenience of accessing your registry via IP addresses.Prepare WireGuard for the serviceAs mentioned, I decided to use WireGuard to achieve point-to-point communication between my DO machine and the container. This set up involves:  Generate private/public key-pairs for the registry service container to use  Add a peer in the DO WireGuard configuration  Set up WireGuard in the registry host (note here we use port forwarding to map registry service’s listening port to the node running it, therefore I can simply set up WireGuard in the node)          For my particular case, I built and deployed the WireGuard module to RancherOS following this great guide                  I did not follow all steps in this guide, and you should decide if having the WireGuard on the host is enough.                    Now let’s assume you configured the WireGuard client with IPv4 address 10.9.60.5.Prepare NGINX for traffic routingI decided to implement basic authentication for my private registry in a reverse proxy that sits in front of the registry. This is simpler than configure the native basic auth registry feature I think, please correct me if I’m wrong.I use simple htpasswd file as an example, but as mentioned in their documentation, any other nginx authentication backend should be fairly easy to implement.Let’s obtain a certificate for the subdomain registry.mxcao.me via Let’s Encrypt:      Make sure your nginx service is not running        Setup your SSL cert, using your method of choice or certbot. If using certbot, first install it:    sudo apt install certbot        Then set it up:    sudo mkdir -p /var/lib/letsencrypt/sudo certbot certonly --email &lt;your@emailaddress&gt; -d registry.mxcao.me --standalone      If successful, you should have your certificate and chain files in /etc/letsencrypt/live/registry.mxcao.meIn your nginx configuration folder (it can be conf.d or sites-available), add below routing rules.Remember to change the docker-registry IP address and the certificate file path to suit your needs.upstream docker-registry {  server 10.9.60.5:5000;}## Set a variable to help us decide if we need to add the## 'Docker-Distribution-Api-Version' header.## The registry always sets this header.## In the case of nginx performing auth, the header is unset## since nginx is auth-ing before proxying.map $upstream_http_docker_distribution_api_version $docker_distribution_api_version {  '' 'registry/2.0';}server {  listen 80;  listen [::]:80;  server_name registry.mxcao.me;  location / {    return 301 https://$server_name$request_uri;  }}server {  server_name registry.mxcao.me;  listen 443 ssl;  listen [::]:443 ssl;  # SSL  ssl_certificate /etc/letsencrypt/live/registry.mxcao.me/fullchain.pem;  ssl_certificate_key /etc/letsencrypt/live/registry.mxcao.me/privkey.pem;  # Recommendations from https://raymii.org/s/tutorials/Strong_SSL_Security_On_nginx.html  ssl_protocols TLSv1.1 TLSv1.2;  ssl_ciphers 'EECDH+AESGCM:EDH+AESGCM:AES256+EECDH:AES256+EDH';  ssl_prefer_server_ciphers on;  ssl_session_cache shared:SSL:10m;  # disable any limits to avoid HTTP 413 for large image uploads  client_max_body_size 0;  # required to avoid HTTP 411: see Issue #1486 (https://github.com/moby/moby/issues/1486)  chunked_transfer_encoding on;  location /v2/ {    # Do not allow connections from docker 1.5 and earlier    # docker pre-1.6.0 did not properly set the user agent on ping, catch \"Go *\" user agents    if ($http_user_agent ~ \"^(docker\\/1\\.(3|4|5(?!\\.[0-9]-dev))|Go ).*$\" ) {      return 404;    }    # To add basic authentication to v2 use auth_basic setting.    auth_basic \"Registry realm\";    auth_basic_user_file /etc/nginx/registry_auth/nginx.htpasswd;    ## If $docker_distribution_api_version is empty, the header is not added.    ## See the map directive above where this variable is defined.    add_header 'Docker-Distribution-Api-Version' $docker_distribution_api_version always;    proxy_pass http://docker-registry;    # required for docker client's sake    proxy_set_header Host $http_host;    # pass on real client's IP    proxy_set_header X-Real-IP $remote_addr;    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;    proxy_set_header X-Forwarded-Proto $scheme;    proxy_read_timeout 900;  }}As you can see in the above configuration, we need to put the password file nginx.htpasswd in /etc/nginx/registry_auth, create this folder if not already exist.For some reason, I can’t make it work following the documentation to generate the password file. Running docker run --rm --entrypoint htpasswd registry:2 -Bbn testuser testpassword &gt; nginx.htpasswd simply not gonna work for nginx to authenticate properly.A work-around is posted here, and to summarize you can run the following commands to populate the nginx.htpasswd password file.sudo sh -c \"echo -n 'testuser:' &gt;&gt; nginx.htpasswd\"# This command will prompt you to enter the pwdsudo sh -c \"openssl passwd -apr1 &gt;&gt; nginx.htpasswd\"So far so good! Start your nginx service and now we are ready to spawn the actual registry service.Set up registry serviceThis is my particular set up case and you can easily port to the docker commands to serve the registry.In my rancher cluster, I added another workload with the following tweaks:  Port mapping from container’s 5000/tcp to host’s 5000/tcp  Env variable REGISTRY_STORAGE_DELETE_ENABLED set to true to enable image deletion  Mount a volume from the host to the container’s path /var/lib/registryDone! Now you can test your private registry set up by running docker login -u=testuser -p=testpwd registry.mxcao.me in your own machine. The expected output should be:WARNING! Using --password via the CLI is insecure. Use --password-stdin.Login SucceededThat’s it! And you can now push unlimited number of repos/images to your own, private registry. Enjoy.",
        "url": "/infrastructure/2020/08/28/private-docker-registry/"
      }
      ,
    
      "notes-2020-08-28-mlops": {
        "title": "MLOps",
        "content": "Introduction  MLOps is an ML engineering culture and practice that aims at unifying ML system development (Dev) and ML system operations (Ops).  MLOps is the natural progression of DevOps in the context of AI… and emphasizes consistent and smooth development of models and their scalability.In simple words, MLOps refers to applying DevOps principles to ML systems.Practicing MLOps means advocating automation and monitoring at all steps (integration, testing, releasing, deployment, infra mngt, etc.) of ML system construction.The goal of MLOps is to build an integrated ML system that can continuously operate in production. As summarized by Google, only a small fraction of a real-world ML system is composed of the actual ML code, and the required surrounding elements are vast and complex, as shown below.MLOps conceptsCI/CD/CT  CI: Continuous Integration          CI in ML no longer only about testing and validating code and components, but also testing and validating data, data schemas, and models.        CD: Continuous Delivery          CD in ML no longer only about a single software package or a service, but a system/pipeline that should automatically deploy another service e.g. model prediction service        CT: Continuous Training          CT is a new property that is concerned with automatically retraining and serving the models (with new/updated data or data stream)      ExampleConsider the typical steps for training and evaluating an ML model to serve as a prediction service.After defined the use cases and established the success criteria, the process of delivering an ML model to production involves:  Data extraction: get relevant data from various sources for the ML task.  Data analysis: perform EDA to understand the extracted data (e.g. schema, characteristics) and identify the data preparation and feature engineering that are needed.  Data preparation/preprocessing: preprocess/clean the extracted data. Typically involves split the data into training/validating/test sets, data transformations, feature engineering. The output are the data splits in the prepared format.  Model training: ML researchers implement algorithms to train various models, perform hyper-parameter tuning, etc. The output is a trained ML model.  Model evaluation: evaluate the trained model quality on a holdout test set. The output is a set of metrics that assess the model quality.  Model validation: confirm that the model is adequate for deployment. In our case this means confirm that its predictive performance is better than a certain baseline.  Model serving: deploy the validated model to a target environment (e.g. as micro services in a k8s cluster, as an embedded model in an edge device, or as part of a batch prediction system) to serve predictions.  Model monitoring: monitor the deployed model’s prediction performance and trigger new iteration in the systemThe above steps can be completed manually by a single team or splitter across different teams (e.g. algorithm team, operation team, etc.), or it can be done by an automatic pipeline.We want to bring automation to the process so that we can benefit from shortened development cycles, increased deployment velocity, and dependable releases, etc.The level of automation of these steps defines the maturity of the ML process and reflects the velocity of model iterations (e.g. triggered by new data or new implementations).MLOps automation levelsLevel 0: manual laborThe below picture shows the typical workflow of this level.Characteristics:  Manual, script-driven, and interactive process  Disconnected b/w ML and operations  Infrequent release iterations  No CI/CD  Deployment is a single service (e.g. prediction) rather than the entire ML system  No active performance monitoringThis approach may be sufficient when models are rarely changed/re-trained. But real-world environment is full of dynamics and models that fail to quickly adapt to changes may decrease in value rapidly.Level 1: ML pipeline automationThe goal of this level is to enable continuous training of the model by automating the ML pipeline, and thus achieve continuous delivery of model prediction service for users.This level of automation typically involves:  Automated data validation  Automated model validation  Pipeline triggers for another iteration  Metadata management (explained later)The following figure is a schematic representation of an automated ML pipeline for CT.Characteristics:  Rapid experiment  CT of the model in production with fresh data based on live triggers  Experimental-Operational symmetry (as seen in the above diagram)  Modularized code for components and pipelines  CD of models (and thus predictive services)  Deployment is a ML pipeline rather than only a prediction serviceTransition from level 0 to level 1To transition to level 1, we need to add new components to the architecture:  Automated data and model validation  Optional feature store: a centralized repo where we standardize the definition, storage, and access of features for training and serving. This is the data source for experimentation, CT, and online serving.  Metadata management: we record information about each execution of the pipeline in order to help with data and artifacts lineage, reproducibility, comparisons, debugging, anomaly detection, etc. Metadata can include:          versioning: of pipeline, or of individual components in the pipeline      timing: start/end date, duration time, etc.      pipeline executor(s)      parameter args      pointer to artifacts produced by each pipeline step (e.g. location of prepared data, computed statistics, etc.)      pointer to previous trained model (this enables model roll-back)      model evaluation metrics (can be thought of as part of the produced pipeline artifacts), which enable model comparison and benchmarking      etc.        Pipeline triggers: e.g. on-data-availability, on-demand, on-schedule, on-model-perf-degradation, on-data-drift, etc.This approach is sufficient if new pipeline implementations are rare deployed and only a few pipelines are managed.The pipeline and its components are usually manually tested and deployed. This is not a good solution if you want to deploy new models based on new ML ideas since manual labor still involved to deploy the pipeline itself, or you are managing many ML pipelines in production.You need a CI/CD setup to automate the build/test/deployment of ML pipelines.Level 2: CI/CD pipeline automationA robust automated CI/CD system allows data scientists rapidly explore new ML ideas around feature engineering, model architecture, hyper-parameters, etc.Data scientists can implement new ideas and the new pipeline will be automatically built, tested, and deployed to the target environment.We can see the updated diagram with CI/CD added for the pipeline.This level typically involves:  Source control  Test and build services  Deployment services  Model registry  Feature store  Metadata management  Pipeline orchestratorCharacteristics:Stages for CI/CD automation pipeline:Manual labors cannot be eliminated for the data analysis and model analysis steps.CIThe pipeline and its components are built, tested, and packaged when new code is committed or pushed to the VCS.E.g. unit testing for the feature engineering logic, for different implemented methods; testing that the model training converges; testing that each component in the pipeline produces the expected artifacts, etc.CDThe new pipeline implementation is continuously deployed to the target environment, and in turn delivers new/updated prediction services.Rapid and reliable pipeline delivery usually involves:  verification of the model compatibility with the target infrastructure before deployment actually happens  test the prediction service with expected inputs and make sure you get expected response within the expected time  test the service performance e.g. QPS, latency, etc.  automated deployment to a test environment  semi-automated deployment to a pre-production environment  manual deployment to a production environment after several successful runs of the pipeline on the pre-production environmentThe following diagram shows the relationship b/w the CI/CD pipeline and the CT pipeline in a ML system:Given new model implementation (e.g. new ML ideas/architecture), a successful CI/CD pipeline deploys a new CT pipeline.Given new data, a successful CT pipeline should serve a new model prediction service.Example: Architecture for MLOps using TFX, Kubeflow Pipelines, and Cloud BuildTFX stands for “TensorFlow Extended” and is an integrated ML platform for developing and deploying production ML systems.A TFX pipeline is a sequence of components that implement an ML system (modeling, training, validation, serving inference, deployment management, etc.).Key libraries of TFX including:  TFT (TensorFlow Transform): data preparation, feature engineering tasks  TFDV (TensorFlow Data Validation): data anomaly detection  TensorFlow Estimators and Keras: model building and training  TFMA (TensorFlow Model Analysis): model evaluation and analysis  TFServing (TensorFlow Serving): serve model in the target environment (e.g. as REST and gRPC APIs)The following diagram shows the architecture of an integrated ML system built from the various TFX libraries (i.e. the design of a TFX-based integrated ML system).With the designed architecture, the next question is how to run each component of the system at scale. Commercial cloud platforms like GCP can help us run the system at scale in a reliable fashion with managed cloud services (e.g. cloud storage, AI hub, dataflow).With the individual components mapped to a managed service in the cloud platform, the next question is how to connect these two pieces together and automate the entire pipeline. An orchestrator performs such tasks and glues our high-level architecture and the underlying individual components. It’s useful for both dev and production phases as it facilitates automation and reduces manual labors.The orchestrator runs the pipeline in sequence and automatically move forward based on the defined conditions (e.g. execute the model serving step after model evaluation finished and the metrics meet predefined thresholds).Kubeflow is the ML Toolkit for Kubernetes. Kubeflow Pipeline is a Kubeflow service that lets you compose, orchestrate, and automate ML systems, where each component of the system can run on various infrastructures (e.g. GCP, local, etc.). Sounds familiar? Yes! It is an orchestrator that we want.A Kubeflow pipeline involves:  A set of containerized tasks/components packed as a docker image. These components can execute any data-related and compute-related services, e.g. Dataproc for SparkML jobs, AutoML, etc.  A sequence of tasks defined by a Python DSL, i.e. the topology of the workflow  A set of pipeline input parametersThe above diagram shows a high-level overview of integrating CI/CD with Kubeflow pipelines in GCP. At the heart of this architecture is Cloud Build, a managed service that executes your builds on GCP. Essentially, the cloud build process performs the required CI/CD for our integrated ML system.The build can be triggered manually or through automated build triggers.For a comprehensive Cloud Build example that covers most of these steps, see A Simple CI/CD Example with Kubeflow Pipelines and Cloud Build.References  MLOps: Continuous delivery and automation pipelines in machine learning  MLOps: What You Need To Know  Architecture for MLOps using TFX, Kubeflow Pipelines, and Cloud Build",
        "url": "/notes/2020/08/28/mlops/"
      }
      ,
    
      "notes-2020-08-19-jekyll-travis": {
        "title": "Use Travis CI for Jekyll site generation",
        "content": "IntroductionI have been using Jekyll for my homepage site generation for months and I’m happy with it. Usually my workflow for posting a new page involves:  Create the source file (.md files)  Commit the changes  Run bundle exec jekyll serve -w  to view the post in my local computer  If satisfied, run JEKYLL_ENV=production bundle exec jekyll build  to generate the public content  Copy the public content to another folder used for publicationAutomation with bash scriptYou can see this is a lot! And posting a trivial page like a short intro or recommendation takes non-trivial time. Later I wrote a bash script that help me handle most of the steps so that I can focus on the content rather than the infra-side things.This solution works pretty well and I stick to it for a while. But… It’s just ugly and I finally decided to bring modern CI tools into the play.Automation with CI/CD toolsI have been using Jenkins during my internship and it seems a good starting point. However, I want to try something different (another kind of exploration). I tried Circle CI this morning and it’s pretty cool. What about Travis? It has been installed in my Github account for a long time and I did not use it before. I finally chose Travis because its interface looks much cleaner and cute.The integration is pretty simple:  Add a .travis.yml config file for Travis-CI to use  Configure the repo in the Travis-CI dashboard  Set up GitHub tokens for the CI bot to be able to deploy my public content in a separate branch (e.g. gh-pages) so that I can simply push the updates to master and let travis to help me update the gh-pages branchJekyll has its own tutorial about setting up Travis CI with GitHub, which is pretty good. You may find my .travis.yml file below as a good starting point if you want to do the same thing too:language: rubyrvm:  - 2.7env:  global:    - NOKOGIRI_USE_SYSTEM_LIBRARIES=true # speeds up installation of html-proofer# branch whitelistbranches:  only:    - master # test the master branchcache: bundler # caching bundler gem packages will speed up buildinstall:  - bundle installscript:  - JEKYLL_ENV=production bundle exec jekyll build  - bundle exec htmlproofer docs --check-html --empty_alt_ignore --http-status-ignore 999 # For now, ignore 999 error from linkedin linksdeploy:  provider: pages  local_dir: docs  target-branch: gh-pages  skip_cleanup: true  github_token: $GITHUB_TOKEN # Set in the settings page of the repository, as a secure variable  keep_history: true  on:    branch: master # Only deploy when current branch is masterNote that I use htmlproofer to check if there’s any broken links. I ignored errors like empty alt attribute and 999 error code from LinkedIn links. You can adjust it for your needs.For the GitHub token, you can reference Travis’s documentation on how to set up the GitHub token for Travis.Thank you and Enjoy.",
        "url": "/notes/2020/08/19/jekyll-travis/"
      }
      ,
    
      "devops-2020-08-18-ansible-101": {
        "title": "Ansible 101",
        "content": "IntroductionThis post was supposed to discuss Ansible as the next primer series. However, I found Jeff’s posts and YouTube videos much better than what I would achive via a plain post. So definitely checkout Ansible 101 by Jeff Geerling. I also recommend purchase the eBook for further reading and as a reference manual for common tasks. Thank you.",
        "url": "/devops/2020/08/18/ansible-101/"
      }
      ,
    
      "primer-2020-08-09-bazel-primer": {
        "title": "Bazel Primer",
        "content": "Introduction 1Bazel is an open-source build/test framework similar to Maven, Make, and Gradle.It features:  Human-readable, high-level build language  Fast and reliable via caching  Scalable  Extensible for other language or frameworkThis post is a reading notes about the official documentation on Bazel version 3.4.0. You can skip these intros and jump directly to the sample repo to get started.Bazel SetupFollow the instructions here to install the latest release for your system.ConceptsIn general, Bazel builds software from source code organized in a directory called a workspace.Source files in the workspace are organized in a nested hierarchy of packages.Each package is a directory containing a set of related source files + one BUILD file for that package.A simple example of a C++ project structure for one package is shown below:.├── README.md├── WORKSPACE└── main    ├── BUILD    └── hello-world.ccWorkspaceA workspace is a directory containing your source files and symbolic links to other directories that contain the build output.Have a look at the following project structure after bazel built the target:.├── README.md├── WORKSPACE├── bazel-bin -&gt; /private/var/tmp/_bazel_mxin/a122b7b4d9e8cf33d3804073143b4e06/execroot/__main__/bazel-out/darwin-fastbuild/bin├── bazel-out -&gt; /private/var/tmp/_bazel_mxin/a122b7b4d9e8cf33d3804073143b4e06/execroot/__main__/bazel-out├── bazel-stage1 -&gt; /private/var/tmp/_bazel_mxin/a122b7b4d9e8cf33d3804073143b4e06/execroot/__main__├── bazel-testlogs -&gt; /private/var/tmp/_bazel_mxin/a122b7b4d9e8cf33d3804073143b4e06/execroot/__main__/bazel-out/darwin-fastbuild/testlogs└── main    ├── BUILD    └── hello-world.ccNote the new symbolic links created from that build.Bazel identify a directory as a workspace root by searching for a file named WORKSPACE or WORKSPACE.bazel. It may be empty or may contain references to external dependencies required to build the outputs. If both WORKSPACE and WORKSPACE.bazel exist, Bazel will ignore the WORKSPACE file.If there’s another subdirectory under the workspace root and it contains a file called WORKSPACE, Bazel simply ignores them. In other words, Bazel does not support nested workspaces.PackagesAs mentioned earlier, source files usually organized in nested hierarchy called packages.Conceptually, a package is  the primary unit of code organization in a repository  a collection of logically related files  a specification of the dependencies among these filesIn reality (ps: joking), it is a subdirectory containing a BUILD or BUILD.bazel file beneath the workspace root. A package includes all files + all subdirectories beneath the package root, except those themselves contain a BUILD (or BUILD.bazel), which become subpackages in this case.For example, the below directory tree contains two packages: my/app and its subpackage my/app/test.src/my/app/BUILDsrc/my/app/app.ccsrc/my/app/data/input.txtsrc/my/app/tests/BUILDsrc/my/app/tests/test.ccRepositoriesIn the above introduction of packages, we mentioned repository, so what is it? We know GitHub repos, and it’s a way of organizing source code. Bazel repository is a similar concept.Bazel defines the root of the main repository as the directory containing the WORKSPACE file, also called @.We can have dependent external repositories like googletest and these external repos are defined in the main repo’s WORKSPACE file using workspace rules.Note that external repos are repos themselves, which means they have their own WORKSPACE file as well! However, these WORKSPACE files are ignored by Bazel and hence those transitively dependent repos are not added automatically.TargetsWithin a package, we define elements as targets. The name of a target is referred as its label.Target categories include:  files  rules  package groups (less nemerous)FilesWe can further divide files as:  Source files          usually written by the efforts of people and checked in to the repo        Generated files (or Derived files)          not checked in to the repo but are generated by the build tool from source files according to specific rules      RulesA rule specifies the relationship between inputs and outputs and the necessary steps to derive the outputs from the inputs.AttributesEach rule has a set of attributes and the applicable attributes for a given rule and the significance/semantics of each attribute are a function of the rule’s class. Each attribute has a name and a type.For example, common attribute types are:  integers  label  list of labels  string  list of strings  output label  list of output labelsNot all attributes need to be specified in every rule (i.e. some attributes are optional). Attributes thus form a dictionary from keys (names) to optional, typed values.Below we introduce several common attributes.name attributeEvery rule has a name attribute of type string and must be syntactically valid target name as explained below (labels section).In some cases, a rule’s name is somewhat arbitrary such as for genrules.In other cases, the name is significant. For example, for *_binary and *_test rules, the name attribute determines the produced executable’s name by the build.srcs attributeThis attribute has type list of labels, which means its value, if present, is a list of labels with each being the name of a target that is an input to this rule.outs attributeThis attribute has type list of output labels. It is similar to the srcs attribute but differs in two significant ways:  due to the invariant that the outputs of a rule belong to the same package as the rule itself (mentioned earlier), output labels cannot include a package component and must be in one of the “relative” forms (discussed below in the labels section)  the relationship implied by an (ordinary) label attribute is inverse to that implied by an output label: a rule depends on its srcs, whereas a rule is depended on by its outs.  The two types of label attributes (srcs and outs) thus assign direction to the edges b/w targets, giving rise to a dependency graph (DAG over targets, a.k.a target graph or build dependency graph), which is the domain over which the Bazel Query tool operates.InputsThe inputs may be source files, generated files, or even other rules. Allowing generated files as the inputs means outputs of one rule may be the inputs to another rule, thus allowing rule chaining. Allowing other rules to be the inputs of one rule is more complex and language/rule-dependent.For example, a C++ library rule A may have another C++ library rule B as input. The effect of this dependency is that B’s header files are available to A during compilation, B’s symbols are available to A during linking, and B’s runtime data is available to A during execution.Note that a rule’s inputs may come from another package.OutputsThe outputs are usually generated files and these files are always belong to the same package as the rule itself.Class (or Categories)A rule can be of one of many different kinds or classes based on the output type. Such as rules that produce compiled executables and libs, test executables and other supported outputs.Package groupsA package group is a set of packages whose purpose is to limit accessibility of certain rules.It is defined by the package_group function and does not generate nor consume files.LabelsAs mentioned in the targets intro above, a target’s name is its label and the label uniquely identifies the target.A typical label in canonical form looks like:@myrepo//my/app/main:app_binaryNote that @myrepo is the repo’s identifier.Usually a label refers to a target in the same repo, and hence we can omit the repo identifier and written it as://my/app/main:app_binaryA label starts with // and consists of two parts separated by a ::  package name          e.g. my/app/main in the above example        target name          e.g. app_binary in the above example      A label’s second part (i.e. the target name) can be omitted if the target name is the same as the last component of the package name. Such short-form labels are just an abbreviation and these two forms are equivalent.For example, if we have label //my/app:app, we can also write it as //my/app.Quick quiz:What are the types of the following representations:  my/app          a package named my/app        //my/app          a target under my/app package, with its label in short-form and target name is assumed to be app        //my/app:app          a target under my/app package, with target name app        @myrepo//my/app/main:app_binary          a target under repo myrepo, package my/app/main, target name app_binary      We can shorten the label identifier even further! Within the BUILD file for package my/app, we can omit the package-name part of labels for this package’s targets, similar to relative paths…For example, if we have targets //my/app, //my/app:app_binary, we can refer to them in the file my/app/BUILD as  //my/app:app or //my/app or :app or app  //my/app:app_binary or :app_binary or app_binaryDon’t be confused with all these forms of representations! Remember to be consistent with your styles of using labels.Usually the colon : is omitted for file targets, but retained for rule targets. This allows us to reference files by their unadorned name relative to the package directory in the package’s BUILD file, e.g.generate.cctestdata/input.txtIf you want to reference targets outside current package in the BUILD file, you need to refer to them using their complete label.For example, with another package named my/test and you want to refer a file in the package my/app in my/test’s BUILD file, you need to use //my/app:generate.cc.If you refer to a target with incorrect label, you may get errors like crosses a package boundary.Labels starting with @// are references to the main repo and still work even from external repos.Therefore @//a/b/c is different from //a/b/c when referenced from an external repo. The former refers back to the main repo while the latter looks for target //a/b/c in the current external repo itself.Such nuance difference can be especially important when you write rules in the main repo that refer to targets in the main repo, but these rules will be used from external repos.I know the label syntax is strict, but Bazel intentionally enforces that to many reasons. The precise details can be found here.The BUILD filesIn the above sections, we discussed packages, targets, labels, build dependency graph abstractly. They are building blocks of Bazel and can be found in a BUILD file.A BUILD file defines a package and is interpreted as a sequential list of statements by using the imperative language called Starlark.By saying “sequential list”, we emphasize the order does matter, especially for variables. Variables must be defined before they are used.In the meantime, the relative order of rule declarations is immaterial and all that matter is which rules were declared and with what value by the time package evaluation completes.So, in simple BUILD files that consist only of rule declarations, these declarations can be re-ordered freely without changing the behavior.Limitations  no function definition, for statements or if statements to encourage a clean separation b/w code and data          functions should be declared in .bzl files instead        no *args or **kwargs arguments          have to list all the arguments explicitly        unable to perform arbitrary I/O          hence the interpretation of BUILD files is hermetic i.e. dependent only on a known set of inputs, which is essential for ensuring that builds are reproducible        should be written using only ASCII charactersBest practices  use comments liberally to document the role of each build target, whether or not it is intended for public use and to document the role of the package itself          since BUILD files need to be updated whenever the dependencies of the underlying code change, and are typically maintained by multiple people on a team      Bazel extensionsBazel extensions are files ending in .bzl.As mentioned in the BUILD file limitations, such files can be used to load new rules, functions or constants. Use load statement in the BUILD file to import a symbol from an extension.E.g. The following code loads the file foo/bar/file.bzl and add the some_library symbol to the environment.load(\"//foo/bar:file.bzl\", \"some_library\")load also supports additional arguments to import multiple symbols.Limitations of load statement:  arguments must be string literals (i.e. no variables)  load statements must appear at the top-level (i.e. cannot be in function body)  the first argument is a label (discussed above) identifying the .bzl file (i.e. a file target). If it is a relative label,          it is resolved w.r.t the package (not directory) containing the .bzl file.      it should use a leading :      Another typical usage of load is to assign different names (i.e. aliases) to the imported symbols:E.g.load(\"//foo/bar:file.bzl\", library_alias = \"some_library\")# multiple symbols and a mix of aliases and regular symbol namesload(\":my_rules.bzl\", \"some_rule\", nice_alias = \"some_other_rule\")In a .bzl file, symbols starting with _ are not exported and thus cannot be loaded from another file.Build rulesMajority of Bazel build rules come in families and grouped by language. For example, cc_binary, cc_library and cc_test are the build rules for C++ binaries, libraries, and tests.As you can imagine, the naming schema for other languages is similar: with a different prefix that identifying that language. E.g. java_* for Java. The suffix identifies the feature of that rule:  *_binary rules build executables. The executable will be put in the build tool’s binary output tree w.r.t the rule’s label, so //my:program will appear at $(BINDIR)/my/program.  *_test rules are a specialization of a *_binary rule and is used for automated testing.          tests return 0 on success      it can only open files that beneath its runfiles tree at runtime        *_library rules specify separately-compiled modules in the given programming language. Libraries can depend on other libs, and binaries and tests can depend on libs.DependenciesWe discussed dependency graph in the above sections, and it models the depends on relationship among targets.A target A depends on a target B if B is needed by A at build or execution time.With the dependency graph defined, we further define a target’s direct dependencies as those direct neighbors in the dependency graph, i.e. targets reachable by a path of length 1 in the DAG. Similarly, a target’s transitive dependencies are those targets on which it depends via a path through the graph.In the context of builds, there are two types of dependency graphs:  the graph of actual dependencies          a target X is actually dependent on target Yif and only if Y must be present, built and up-to-date in order for X to be built correctly.                  built could mean generated, processed, compiled, linked, archived, compressed, executed, or any other kinds of tasks that routinely occur during a build.                      the graph of declared dependencies          a target X has a declared dependency on target Y if and only if there’s a dependency edge from X to Y in the package of X.      In order to have a correct build, the actual dependency graph (denoted by Α) must be a subgraph of the declared dependency graph (denoted by D) (i.e. every pair of directly-connected nodes in A must also be directly connected in D). We therefore say D is an overapproximation of A.What all these mean is that BUILD file writers should try to make D as close to A as possible, and thus every rule must explicitly declare all of its actual direct dependencies to the build system, and no more.Types of dependenciesMost build rules have 3 attributes for specifying different kinds of generic dependencies: srcs, deps, and data. Other attributes also exist for rule-specific kinds of dependencies e.g. compiler, resources, etc.  srcs dependencies          represent files directly consumed by the rule or rules that output source files        deps dependencies          rule pointing to separately-compiled modules providing header files, symbols, libraries, data, etc.        data dependencies          the build system runs tests in an isolated directory where only files listed as data are available              E.g.        # I need a config file from a directory named env:java_binary(  name = \"setenv\",  ...  data = glob([\"testdata/**\"]),)# I need test data from another directorysh_test(  name = \"regtest\",  srcs = [\"regtest.sh\"],  data = [    \"//data:file1.txt\",    \"//data:file2.txt\",    ...  ],)            Example projectI tried to re-build our previous post Value-Parameterized GTest with Bazel, and you can find the source code here. It’s interesting to compare these two branches (master branch uses cmake while bazel branch uses bazel) and appreciate the elegance when we adopted Bazel.            https://www.bazel.build &#8617;      ",
        "url": "/primer/2020/08/09/bazel-primer/"
      }
      ,
    
      "primer-2020-08-08-wireguard-primer": {
        "title": "WireGuard Primer",
        "content": "IntroductionWireGuard 1 is a simple, fast, and modern VPN that utilizes cryptography.It encapsulates IP packets over UDP. The general workflow can be summarized as:  Install WireGuard  Add a WireGuard interface (like wlan0/eth0 but named wg0/wg1…)  Configure the interface with a private key and optionally add peer’s public key(s)  Send packets to the interfaceIt’s similar to the ssh model. Both parties have each other’s public keys and they communicate by exchanging packets encrypted by the public keys through the WireGuard interface.About the WireGuard network interfaceThe WireGuard network interface is where the magic happens and it acts as a tunnel interface.You can add one or more such interfaces depending on your needs. It can be configured normally using utilities like ifconfig(8) or ip-address(8), but more typically you configure such interfaces using the wg(8) tool for WireGuard-specific settings.By saying that it acts as a tunnel interface, we mean:  It associates tunnel IP address with public keys and remote endpoints  Sending a packet to a peer through a WireGuard interface involves          Find the destination peer                  E.g. this packet is meant for 192.168.30.8. Which peer is that? Let me look… Okay, it’s for peer ABCDEFGH. (Or if it’s not for any configured peer, drop the packet.)                    Encrypt the IP packet using peer ABCDEFGH’s public key      Find the destination’s remote endpoint                  E.g. what is the remote endpoint of peer ABCDEFGH? Let me look… Okay, the endpoint is UDP port 53133 on host 216.58.211.110                      Receiving a packet from a peer through a WireGuard interface involves          Decrypt the received packet                  I just got a packet from UDP port 7361 on host 98.139.183.24. Let’s decrypt it!          If the decryption failed, drop the packet; Otherwise let’s remember that peer LMNOPQRS’s most recent Internet endpoint is 98.139.183.24:7361 using UDP                    Authentication                  Now we have the plaintext packet from peer LMNOPQRS with IP 192.168.43.89, is LMNOPQRS allowed to be sending packets to us as 192.168.43.89?          If so, accept the packet on the interface. If not, drop it.                    The above short, simplified summary is for our understanding purpose and there’s much happening behind the scenes to provide proper privacy, authenticity, and secrecy.About Cryptokey RoutingCryptokey Routing is at the heart of WireGuard.It associates public keys with a list of tunnel IPs that are allowed inside the tunnel. Those public keys correspond to a list of peers, each peer has a public key. We use public keys to authenticate each other’s identity. They can be safely passed around for use in plaintext config files by any out-of-band method.It also associates a private key with a WireGuard network interface.For example, a server may have below config file:[Interface]PrivateKey = yAnz5TF+lXXJte14tji3zlMNq+hd2rYUIgJBgB3fBmk=ListenPort = 51820[Peer]PublicKey = xTIBA5rboUvnH4htodjb6e697QjLERt1NAB4mZqp8Dg=AllowedIPs = 10.192.122.3/32, 10.192.124.1/24[Peer]PublicKey = TrMvSoP4jYQlY6RIzBgbssQqY3vxI2Pi+y71lOWWXX0=AllowedIPs = 10.192.122.4/32, 192.168.0.0/16[Peer]PublicKey = gN65BkIKy1eCE9pP1wdc8ROUtkHLF2PfAqYdyYBz6EA=AllowedIPs = 10.10.10.230/32Each peer in this case will be a client that can send packets to the server’s network interface with a source IP matching the allowed IPs.For example, if a packet is received by the server from peer gN65BkIK..., after being decrypted and authenticated, if its source IP is 10.10.10.230, then it’s allowed onto the interface; otherwise it’s dropped. If the server wants to send a packet to a client, it looks at that packet’s destination IP and compares it to each peer’s list of allowed IPs to see which peer to send it to. For example, if the server’s network interface is asked to send a packet with a destination IP of 10.10.10.230, it will encrypt it using the public key of peer gN65BkIK..., and then send it out to the peer’s most recent internet endpoint.In the meantime, a client may have the below config file:[Interface]PrivateKey = gI6EdUSYvn8ugXOt8QQD6Yc+JyiZxIhp3GInSWRfWGE=ListenPort = 21841[Peer]PublicKey = HIgo9xNzJMWLKASShiTqIybxZ0U3wGLiUeJ1PKf8ykw=Endpoint = 192.95.5.69:51820AllowedIPs = 0.0.0.0/0The client here has only 1 peer with public key HIgo9.... That peer is able to send packets to this interface with any source IP.The endpoint in the above client config is an initial endpoint of its single peer so that it knows where to send encrypted data before it has received encrypted data. Compared to this, the server side config does not have any initial endpoints of its peers because the server discovers the peer endpoints by examining from where correctly authenticated data originates.If a server changes its own endpoint, and sends data to the clients, the clients will discover the new server endpoint and update its config. This is what we referred in the previous discussion “use the most recent IP endpoint”. Such feature is also called IP roaming.For example, if a packet is received from peer HIgo9..., after being decrypted and authenticated, it is allowed onto the interface; otherwise it is dropped. In the meantime, if the client’s network interface is asked to send a packet to its single peer, it will encrypt the packet for the peer with any destination IP since the allowed IP is a wildcard 0.0.0.0/0, and then send it out to the single peer’s most recent internet endpoint.In other words, the list of allowed IPs behave differently depending on the purpose is sending or receiving:  When sending packets out, the list behaves like a routing table that determines which host to send the packets to  When receiving packets, the list behaves like a access control list (ACL) that determines whether to keep the packets or notThis is important and keep these two rules in mind!Combining the public key and the bi-purpose list of allowed IPs is what we call a CryptoKey Routing Table: the simple association of public keys and allowed IPs.Note that any of the IP field can be either IPv4, IPv6, or any combination of them.The tight coupling of peer identity and the allowed IPs ease system admins from complicated firewall extensions but rather using a simplified match on “is it from this IP? on this interface?”.SetupInstallation 2Here I only cover the commands used to install WireGuard in Ubuntu.sudo apt install wireguardVerify your installation by checking its version:wg -vVerify that the wireguard kernel module is loaded:lsmod | grep wireguardIf not loaded, try to reboot your instance and check again.You can manually load it with:sudo modprobe wireguardServer Setup 3Generate a public and private certificate on the server:umask 077wg genkey | tee server_private_key | wg pubkey &gt; server_public_keyCreate the server config file in /etc/wireguard/wg0.conf:[Interface]Address = 10.9.0.1/24, fd42:42:42::1/64SaveConfig = truePrivateKey = # PLACEHOLDER for your server side private keyListenPort = 51820PostUp = iptables -A FORWARD -i %i -j ACCEPT; iptables -A FORWARD -o %i -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE; ip6tables -A FORWARD -i %i -j ACCEPT; ip6tables -A FORWARD -o %i -j ACCEPT; ip6tables -t nat -A POSTROUTING -o eth0 -j MASQUERADEPostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -D FORWARD -o %i -j ACCEPT; iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE; ip6tables -D FORWARD -i %i -j ACCEPT; ip6tables -D FORWARD -o %i -j ACCEPT; ip6tables -t nat -D POSTROUTING -o eth0 -j MASQUERADE[Peer]PublicKey = # PLACEHOLDER for your client side public keyAllowedIPs = 10.9.0.2/32, fd42:42:42::2/128Note that you need to change the public interface based on your situation. In my case it is eth0. If it is not, change it to the actual name.As we mentioned earlier, the list of AllowedIPs has different interpretation when sending/receiving packets (a routing table when sending and a ACL when receiving). You will want to add your LAN’s subnet under AllowedIPs so that you can access them through the tunnel. Note that it is a subnet! Otherwise you may encounter errors like RTNETLINK answers: File exists.In the above sever config file, I also assigned an IPv6 IP for the server and the peer since my instance has a public IPv6 address, and hence we can use it as a dual-stack VPN.Enable IPv4/IPv6 forwardingOpen /etc/sysctl.conf and uncomment the following lines:net.ipv4.ip_forward=1net.ipv6.conf.all.forwarding=1Restart the server or use the following commands to let the forwarding to take effect.sudo sysctl --systemStart WireGuardsudo chown -v root:root /etc/wireguard/wg0.confsudo chmod -v 644 /etc/wireguard/wg0.confwg-quick up wg0sudo systemctl enable --now wg-quick@wg0.serviceClient SetupI’m using the WireGuard Mac App, and the configuration is simple and intuitive. Follow the instructions above to generate the certificates for the client, and populate the configs following the official docs.Forward all your traffic through the tunnel 4Remember what we emphasized earlier? The dual-purpose list of AllowedIPs? If you want your traffic looks like it’s coming from your server, you can forward all the traffic through WireGuard interface.Simply change the AllowedIPs line on the client to this:AllowedIPs = 0.0.0.0/0, ::/0This will make the wg0 (or whatever your WireGuard interface is) responsible for routing all IP addresses/traffic over your server. You can check this by visiting ping.pe.Forward all IPv6-traffic through the tunnelI want to route all my IPv6 traffic through the tunnel but keep the IPv6 traffic untouched. At first I tried setting:AllowedIPs = 10.9.0.1/32, ::/0But it seems WireGuard also changed the routing table for IPv4 (checked via netstat -nr). I found this reddit post and the workaround is:AllowedIPs = 10.9.0.1/32, ::/1, 8000::/1It’s a clever hack to make it work but I expect a cleaner solution should exist (like the previous config).Now I can reach IPv6-only websites like bt.byr.cn while not tunneling my IPv4 traffic to the slow VPN network!Protect your DNS 5When you use WireGuard in your machine as a client, your local network won’t be accessible, which means if the DNS servers pushed by your DHCP server are in the local network, you cannot access it! You can add a DNS entry in your client interface config like below:[Interface]PrivateKey = (hidden)Address = 10.9.0.2/24, fd42:42:42::2/64DNS = 176.103.130.130, 176.103.130.131[Peer]PublicKey = # PLACEHOLDER for your peer's public keyAllowedIPs = 10.9.0.1/32, fd42:42:42::1/128Endpoint = # PLACEHOLDER for your peer's remote endpoint, be it IPv4 or IPv6Here we use Adguard DNS but you can definitely host your own DNS server in your server machine. I also recommend NextDNS for your DNS experience. Thank you.            https://www.wireguard.com/ &#8617;              https://www.wireguard.com/install/ &#8617;              http://portal.altispeed.com/kb/faq.php?id=201 &#8617;              https://www.stavros.io/posts/how-to-configure-wireguard/ &#8617;              https://stanislas.blog/2019/01/how-to-setup-vpn-server-wireguard-nat-ipv6/ &#8617;      ",
        "url": "/primer/2020/08/08/wireguard-primer/"
      }
      ,
    
      "primer-2020-08-07-value-parameterized-gtest": {
        "title": "Value-Parameterized GTest",
        "content": "IntroductionLast time I talked about gtest in the gtest primer, and in this post we discuss the usage of “value-paramterized” tests. In general, when you write a simple test case and verified it is working, what is the next step? Maybe you want to add more test cases! But how? Adding them by “hard-coding” the logics in the source files? It’s not a good idea since cases may evolve and we may later add new corner cases, maybe months later. At that time, you may already forgot how to write a test case using gtest.Value-parameterized tests are a good way to organize your test cases and group them in a logical manner e.g. test cases for a particular API. I use value-parameterized tests and a YAML configuration file to dynamically add/reduce my test cases, so that adding or removing a test case is as simple as updating the YAML file.Problem settingI finished writing test cases for the new APIs using gtest and I felt great! During the meeting, my mentor raised a question: “How can I add a new test case when you finished your internship? Do I need to learn gtest from scratch to do that?”. Definitely “yes” is not a good option, so I decided to update existing test implementations and use an easier approach to add/remove test cases. That is using value-parameterized tests together with a test config file written in YAML.An example test config file should be like:Foo:  # Read-oriented API tests  GetBarTest:    - case_name: TestEmptyParams      case_assertion: EXPECT_EQ      case_description: \"Test if right value returned when no arg is given\"      case_params:        name: \"barbar\"        verbose: true      case_expected_result: []Prepare the APIsIn this example we will add a simple getter method under the namespace of “Foo”. The only function we have is called GetBar which returns a vector of strings.The header file foo.h contains:namespace foo {std::vector&lt;std::string&gt; GetBar();}  // namespace fooAnd its implementation foo.cpp is:#include \"foo.h\"namespace foo {std::vector&lt;std::string&gt; GetBar() {  return std::vector&lt;std::string&gt;{\"hello\", \"world\"};}}  // namespace fooSetup the gtestI want to be as general as possible. So I setup a separate fixture for namespace Foo. If we later added more APIs under Foo, then the adaptation will be easier.Note that we use yaml-cpp to parse the config files and I will assume you already built and installed the library.I created a header file called test_foo_core.h to setup the test cases for Foo and defined a value-parameterized fixture class called FooTest. All the API tests can afterwards be derived from the FooTest class to reuse the settings. The value type will be a YAML::Node that include the configurations for a specific test case.The SetUp method of FooTest will retrieve the YAML::Node by using gtest’s GetParam method, from which we read the case_params and case_assertion.We defined a function ReadTestCasesFromYaml to help us parse a specific test case from the test config file. You provide the fucntion test name (e.g. GetBarTest in the above example config file) and it will read all the test cases under that name and use them to run the tests.Enough introduction and explanations! Go clone the tutorial repo and play around the tests!Commands to run the build/test/coverageIn the tutorial repo, I also included the option to produce a code coverage report using gcov and lcov. Check it out by running make coverage under the build directory and read the reports in the coverage folder.Clone the repogit clone git@github.com:mcao2/value-parameterized-gtest.gitBuild the source files and testsmkdir build &amp;&amp; cd buildcmake -DBUILD_TEST=ON .. &amp;&amp; make -j4Run the gtestctest -VSample outputConstructing a list of testsDone constructing a list of testsUpdating test list for fixturesAdded 0 tests to meet fixture requirementsChecking test dependency graph...Checking test dependency graph endtest 1    Start 1: test_foo1: Test command: value-parameterized-gtest/build/test/test_foo1: Environment variables:1:  TEST_FOO_CONFIG_PATH=value-parameterized-gtest/test/test_foo.yaml1:  GTEST_OUTPUT=xml:value-parameterized-gtest/build/reports/gtest_test_foo.xml1: Test timeout computed to be: 100000001: [GetBarTest] Num cases: 11: Read case TestEmptyParams1: [==========] Running 1 test from 1 test suite.1: [----------] Global test environment set-up.1: [----------] 1 test from GetBarTestInstantiation/GetBarTest1: [ RUN      ] GetBarTestInstantiation/GetBarTest.GetBar/TestEmptyParams1: [       OK ] GetBarTestInstantiation/GetBarTest.GetBar/TestEmptyParams (0 ms)1: [----------] 1 test from GetBarTestInstantiation/GetBarTest (0 ms total)1:1: [----------] Global test environment tear-down1: [==========] 1 test from 1 test suite ran. (1 ms total)1: [  PASSED  ] 1 test.1/1 Test #1: test_foo .........................   Passed    0.48 secCreate code coverage reportmake coverageTo view the report, go to coverage and run:python3 -m http.serverAnd you can view your report from 127.0.0.1:8000.Sample outputThank you and I hope this is useful :)",
        "url": "/primer/2020/08/07/value-parameterized-gtest/"
      }
      ,
    
      "primer-2020-07-19-git-primer": {
        "title": "Git Primer",
        "content": "Git PrimerGit Commit MessagesGit commit messages can help you understand why something happened months or years ago for your collaborators or yourselves. It affects a project’s maintainability, and hence plays a critical role in a project’s long-term success.This section we summarize some common techniques to write a healthy commit message. Of course there are variations on the styles and conventions adopted by different projects/orgs, you should pick one and stick to it as much as possible to reduce chaos and inconsistency.A commit message is nothing but some plain texts explaining the changes you made. We can describe a git commit msg convention from several perspectives:  Style: e.g. capitalization, punctuation  Content: info that the commit msg body should or should not contain  Metadata: e.g. reference PR numbers, issue tracking IDsFortunately, well-established conventions exist and you don’t need to re-invent the wheel.Several rules of a great git commit msg 1  Separate subject from body with a blank line  Limit the subject line to 50 characters  Capitalize the subject line  Do not end the subject line with a period  Use the imperative mood in the subject line  Wrap the body at 72 characters  Use the body to explain what and why vs. howE.g.Summarize changes in around 50 characters or lessMore detailed explanatory text, if necessary. Wrap it to about 72characters or so. In some contexts, the first line is treated as thesubject of the commit and the rest of the text as the body. Theblank line separating the summary from the body is critical (unlessyou omit the body entirely); various tools like `log`, `shortlog`and `rebase` can get confused if you run the two together.Explain the problem that this commit is solving. Focus on why youare making this change as opposed to how (the code explains that).Are there side effects or other unintuitive consequences of thischange? Here's the place to explain them.Further paragraphs come after blank lines. - Bullet points are okay, too - Typically a hyphen or asterisk is used for the bullet, preceded   by a single space, with blank lines in between, but conventions   vary hereIf you use an issue tracker, put references to them at the bottom,like this:Resolves: #123See also: #456, #789Notes      Not every commit msg needs both a subject and a body and sometimes a single line is fine for simple changes    E.g.     git commit -m \"Fix typo in introduction to user guide\"            Commit msgs with a subject and a body are not easy to write with the -m option. You should configure an editor for use with git. You can also define a commit msg template in the git config like below to remind you of the proper format and style when creating a commit msg:    E.g. Consider a template file at ~/.gitmessage.txt that looks like this     Subject line (try to keep under 50 characters) Multi-line description of commit, feel free to be detailed. [Ticket: X]        To tell Git to use it as the default message that appears in your editor when you run git commit, set the commit.template configuration value:     $ git config --global commit.template ~/.gitmessage.txt $ git commit            The blank line b/w the subject and body is essential and there’re a number of contexts in git like git log --oneline or git shortlog that the distinction b/w subject and body matter        The 50 characters limit for the subject line forces the author to think about the most concise way to express his ideas. It also ensures that they are readable from the GitHub WebUI        Your subject line should always be able to complete the following sentence:          If applied, this commit will your subject line here            When you write the git msg body if needed, be mindful about its right margin and wrap text at 72 characters. Consider to set up a good text editor that automatically wrap texts for you    If you are using VSCode, the below settings can be a starting point:                  Set VSCode as your default editor for git          # '--wait': Wait for the files to be closed before returning.  $ git config --global core.editor \"code --wait\"                            In your VSCode user settings.json, add the following wrap rule:          \"[git-commit]\": {      \"editor.rulers\": [          72      ],      \"editor.wordWrapColumn\": 72,      \"editor.wordWrap\": \"wordWrapColumn\",      \"editor.wrappingIndent\": \"same\"  }                More customizations can be found here.            Git Commands 2  Commit changes          git commit -m 'Simple msg goes here'      git commit --amend 3                  Sometimes we commit too early and possibly forget to add some files, or you mess up your commit msg format, you want to undo that commit, make additional changes you forgot, stage and commit again. Now this time use the --amend option!          This will take you staging area and uses it for the commit. The same commit msg will pop up and you can update this msg and it will overwrites your previous commit          You end up with a single commit, the one amended will replace the results of the first                      Create a new branch          git branch bugFix (Note that this does not set the HEAD at the new branch)      git checkout -b bugFix        Combine changes          merge                  Create a special commit that has two unique parents:                                          git merge bugFix                                            If you want to keep the bugFix branch up to date and continue work on it, we can merge the master branch to the bugFix branch. Since bugFix was an ancestor of master after the above merge, it simply move the bugFix to the same commit as master: git checkout bugFix; git merge master                                              rebase                          Explanations:                                  Take a set of commits, copy them and paste them down somewhere else. It can be used to make a nice linear sequence of commits and the commit history will be much cleaner.                  Take all the changes that were committed on one branch and reply them on a different branch.                                            git rebase target_branch_to_reply_changes_to                                  E.g. If you are on branch bugFix and want to move work from bugFix directly onto the master. That way the commit history will looks like these two features were developed sequentially, when in reality they were developed in parallel.                  git checkout bugFix; git rebase master                                          Now you can go back to the master and do a fast-forward merge to keep the master up to date: git checkout master; git merge/rebase bugFix                                                                                                                Moving around commits          HEAD: the symbolic name for the currently checked out commit. It always points to the most recent commit which is reflected in the working tree.      Detaching HEAD: attach HEAD to a commit instead of a branch, e.g. when you checkout a particular commit rather than a branch. Typically you find the commit by using git log to find the right hash.                  Relative Refs                          Start from somewhere memorable e.g. branch bugFix or the HEAD, and move from there                                  moving upwards, 1 commit at a time: ^ (Caret operator)                                          e.g.                                                  detach head to its parent: git checkout HEAD^1, or                          checkout to the master’s grandparent: git checkout master^^                                                                                                      moving upwards for &lt;num&gt; commits: ~&lt;num&gt; (Tilde operator)                                          e.g.                                                  detach head and move upward 4 commits: git checkout HEAD~4                                                                                                                                Branch forcing:                                  Reassign a branch to a particular commit                  E.g. move the master branch to 3 parents behind HEAD: git branch -f master HEAD~3                                                                          Reversing changes          git reset                  Rewriting history: revert changes by moving a branch reference backwards in time to an older commit          Warning: This cmd is great for local branches on your own machine, not good for remote branches that others are using!          E.g. move the master branch to the parent of HEAD: git reset HEAD^, or you can achieve this via branch forcing: git branch -f master HEAD^          Modes                          --soft                                  does not touch the index file or the working tree at all but resets the head to the given &lt;commit&gt;                                            --mixed                                  default mode, resets the index file but not the working tree, i.e. preserve changes but not marked for commit                                            --hard                                  resets the index and working tree and any changes to tracked files are discarded                                            --merge              --keep                                          git revert                  Revert changes and share those reversed changes with others by creating a new commit, whose effect is to revert a particular commit                      Move works around          i.e. “Move this commit here, that commit there”      git cherry-pick &lt;commit1&gt; &lt;commit2&gt; &lt;...&gt;                  A way of saying that you want to copy the provided commits below your current location i.e. below the HEAD          Surely you may be able to achieve the same goal by using git rebase, but cherry-pick is kinda of magic and easier to achieve this goal…                    Interactive rebasing git rebase -i target_branch_to_replay_changes_to                  Cover the situation where you don’t know what commits you want: review a series of commits you’re about to rebase          Git will open up a UI to show you which commits are about to be copied below the target of the rebase (i.e. target_branch_to_replay_changes_to).                          You can reorder commits by changing their order in the UI              You can choose to completely omit some commits              And many more…                                          References            https://chris.beams.io/posts/git-commit/ &#8617;              https://learngitbranching.js.org/?locale=en_US &#8617;              https://git-scm.com/book/en/v2/Git-Basics-Undoing-Things &#8617;      ",
        "url": "/primer/2020/07/19/git-primer/"
      }
      ,
    
      "primer-2020-07-12-boost-log-primer": {
        "title": "Boost.Log Primer",
        "content": "Boost Log 1Prepare the namespace aliasesnamespace logging = boost::log;namespace sinks = boost::log::sinks;namespace src = boost::log::sources;namespace expr = boost::log::expressions;namespace attrs = boost::log::attributes;namespace keywords = boost::log::keywords;Definitions 2  Log record: a single bundle of info that is collected from the app and is a candidate for the log output  Attribute: a piece of meta-info that can be used to specialize a log record e.g. ThreadID, Timestamp, etc.  Attribute value: the actual data acquired from attributes and is attached to a specific log record. Values can have different types  Attribute value visitation: a way of processing the attribute value, which involves applying a “visitor” (a function obj) to the attribute value  Attribute value extraction: a way of processing the attribute value when the caller attempts to obtain a reference to the stored value. The caller should know the stored type of the attr value in order to be able to extract it  Log sink: a target to which all log records are fed after being collected from the user’s app  Log source: an entry point for the user’s app to put log records to  Log filter: a predicate that takes a log record and tells whether this record should be kept or discarded  Log formatter: a function obj that generates the final textual output from a log record  Logging core: the global entity that maintains connections b/w sources and sinks and applies filters to records. usually used at the logging init  TLS: thread-local storageArchitecture 3Three-layer architecture  Data collection layer  (Connection layer) Central-hub that interconnects the collection and the processing layers  Data processing layerVisualization  Arrows show the info flow direction          the left-most is our application (log sources)      the right-most is the final storage if any                  storage is optional because the processed log may include some actions without actual data persisting          e.g. the app can emit a special log record that will be processed s.t. the user sees the error msg as a tool-tip notification over the app icon and hears an alarming sound          this allows boost log be used not only for classic logging, but also to indicate some important events to the app user and accumulate statistical data                    Log sourcesA log source is an entity that initiates logging by constructing a log record. In general, boost log lib does not require the use of loggers to write logs. But loggers are the most common kind of log sources.In the left-most of the arch, our application emits log records with the help of loggers, which are special obj that provide streams to format msgs that will eventually be put to logBoost log lib provides many different logger types and you can craft your own by extending the existing ones.Loggers are designed as a mixture of distinct features s.t. they can be used in combination.You can embed the logger into your application classes or create and use a global instance of the logger.  Embedding a logger provides a way to differentiate logs from different instances of the class  A single global logger instance is more convenient like in functional-style programmingAttributes and attribute valuesA log source must pass all data associated with the log record to the logging core. This data is represented with a set of named attributes and each one is a function whose result is what we call “attribute values”. The attribute values are processed on further stages.An example attribute is a function that returns the current timestamp, and its result (i.e. its value) is the particular time point.We can classify attributes into 3 categories, ordered by increasing priority:  Global attributes  Thread-specific attributes  Source-specific attributesThe global and thread-specific attributes are maintained by the logging core, as shown in the architecture figure, and therefore these attributes need not be passed by the log source in order to initiate logging.The global attributes are attached to any log record ever made.Thread-specific attributes are attached only to the records made from the thread in which they were registered in the set.Source-specific attributes are maintained by the source that initiates logging, and attached only to the records being made through that particular source.When a log source initiates logging, it acquire attribute values from all three attribute sets. These values form a single set of named attribute values, and is then processed further.It’s possible that a same-named attribute that appear in several attribute sets. Such conflicts are solved on priority basis. The global attributes have the least priority and the source-specific attributes have the highest.Logging core and filteringFiltering: After the logging source composed the named attribute values set, the logging core decides if this log record is going to be processed in sinks.Two layers of filtering:  Global filtering is applied first within the logging core and allows quickly wiping away unneeded log records  Sink-specific filtering is applied second within each sink separately and allows directing log records to particular sinks.It is not significant where the log record comes from (i.e. regardless the logging source that emit them), the filtering relies solely on the set of named attribute values attached to the record.For a given log record, the filtering is performed only once and only those attribute values that are attached to the record before the filtering starts can participate in filtering. We say this because some named attribute values, like the “log record message”, are typically attached to the log record after the filtering is done, which means these values cannot be used in filters!Sinks and formattingA log record that passes filtering for at least one sink is considered as consumable.If the sink supports formatted output, this is the point when log msg formatting takes place. Note that formatting is done on a per-sink basis s.t. each sink can have its own specific output format.The formatted msg along with the composed set of named attribute values are passed to the accepting sinks.As shown in the arch figure, a sink consists of two parts:  Sink frontend  Sink backendThe division is made to extract the common functionality of sinks like filtering, formatting, thread sync into separate entities (frontends).Sink frontends are provided by the lib and usually users won’t have to reimplement them.Sink backends are one of the most likely places for extending the library. The backends do the actual data processing.There can be multiple sinks in a single app.E.g.  a sink that stores processed log records into a file  another sink that sends the processed log records over the network to the remote log processing node  another sink that puts record msgs into tool-tip notificationsThe lib also provides most commonly used sink backends.Tutorial 4Trivial loggingNecessary headers:#include &lt;boost/log/trivial.hpp&gt;The BOOST_LOG_TRIVIAL macro accepts a severity level and results in a stream-like objects that support insertion operator. The log msg will be printed on the console.E.g.#include &lt;boost/log/trivial.hpp&gt;int main(int, char*[]){    BOOST_LOG_TRIVIAL(trace) &lt;&lt; \"A trace severity message\";    BOOST_LOG_TRIVIAL(debug) &lt;&lt; \"A debug severity message\";    return 0;}Features:  Each log record in the output msg contains a timestamp, a thread ID and the severity level  It is safe to write logs from multiple threads concurrently and log msgs will not be corrupted  Filtering can be appliedAdd filters to the trivial loggingYou will normally want to apply filters to output only significant records and ignore the rest.Let’s do this by setting a global filter in the logging core.void init(){    logging::core::get()-&gt;set_filter    (        logging::trivial::severity &gt;= logging::trivial::info    );}int main(int, char*[]){    init();    BOOST_LOG_TRIVIAL(trace) &lt;&lt; \"A trace severity message will be ignored\";    BOOST_LOG_TRIVIAL(debug) &lt;&lt; \"A debug severity message will be ignored\";    BOOST_LOG_TRIVIAL(info) &lt;&lt; \"An informational severity message\";    return 0;}Since we are setting up a global filter, we have to acquire the logging core instance using logging::core::get(). It returns a pointer to the core singleton instance.The filter in this example is built as a Boost.Phoenix lambda expression. The left argument is a placeholder that describes the attribute to be checked. The placeholder along with the ordering operator creates a function object that will be called by the logging core to filter log records and only records that pass the predicate will end up on the console.Setting up sinksThe lib contains a default sink that is used as a fallback when the user did not set up any sinks. This is why our previous trivial logging example worked. This default sink always print log records to the console in a fixed format and is mostly provided to allow trivial logging to be used right away.Sometimes trivial logging does not provide enough flexibility like when one wants to apply complex log processing logic. You need to construct logging sinks and register them with the logging core in order to customize this. This only needs to be done once somewhere in the startup code.Note that once you add any sinks to the logging core, the default sink will no longer be used! You will still be able to use trivial logging macros though.Logging to filesE.g.void init(){    logging::add_file_log(\"sample.log\");    logging::core::get()-&gt;set_filter    (        logging::trivial::severity &gt;= logging::trivial::info    );}The add_file_log function initializes a logging sink that stores log records into a text file. You can also customize it with additional parameters in a named form:logging::add_file_log(    keywords::file_name = \"sample_%N.log\", // file name pattern    keywords::rotation_size = 10 * 1024 * 1024, // rotate files every 10 MiB    keywords::time_based_rotation = sinks::file::rotation_at_time_point(0, 0, 0), // or at midnight    keywords::format = \"[%TimeStamp%]: %Message%\" // log record format);Creating loggers and writing logsThe logger object is a logging source that can be used to emit logs records.The trivial logging example we seen above uses the logger provided by the lib and is used behind the scenes through the macro.Unlike sinks, logging sources (loggers) need not be registered since they interact with the logging core directly.Boost log lib provides two types of loggers:  Thread-safe loggers  Non-thread-safe loggersIt is safe for different threads to write logs through different instances of the non-thread-safe logger, which means a separate logger for each thread to write logs.The thread-safe logger can be accessed from different threads concurrently and the thread-safety is protected by using locks, which means worse performance incase of intense logging compared to the non-thread-safe counterparts.Regardless of the thread safety, all lib-provided loggers are default and copy-constructible and support swapping, so there should be no problem in making a logger a member of your class.The lib provides many loggers with different features, such as severity and channel support. These features can be combined with each other in order to construct more complex loggers.Global logger objectIn case you cannot put a logger into your class (suppose you don’t have one), the library provides a way of declaring global loggers like this:// Remember that we defined namespace src = boost::log::sources;BOOST_LOG_INLINE_GLOBAL_LOGGER_DEFAULT(my_logger, src::logger_mt)The my_logger is a user-defined tag name that will be used later to retrieve the global logger object.The logger_mt is the logger type. Any logger type provided by the lib or defined by the user can participate in such declaration. But note that you will normally want to use thread-safe loggers in a multi-threaded app as global logger obj since the logger will have a single instance.In other parts of your app, you can acquire the global logger like this:src::logger_mt&amp; lg = my_logger::get();The lg will refer to the one and only instance of the global logger throughout the application. The get function is thread-safe and hence you don’t need additional synchronization around it.Writing logsE.g.logging::record rec = lg.open_record();if (rec){    logging::record_ostream strm(rec);    strm &lt;&lt; \"Hello, World!\";    strm.flush();    lg.push_record(boost::move(rec));}The open_record function determines if the record to be constructed is going to be consumed by at least one sink, i.e. filtering is applied at this stage.If the record is to be consumed, the function returns a valid record obj and we can then fill the record msg into it. The record processing can be completed with the call to push_record.It seems complicated to use the logger even for the most simple log message… But we can easily wrap them into a macro! The log record above can be written like this:BOOST_LOG(lg) &lt;&lt; \"Hello, World!\";The BOOST_LOG macro, along with other similar ones, is defined by the library.Adding more info to log with AttributesAs said above, each log record can have many named attribute values attached. Attributes contain essential information about the log emit condition, like the line number in the code, executable name, current time, etc.An attribute may behave as a value generator, i.e. it may return a different value for each log record it is involved in. As soon as the attribute generates the value, the value becomes independent from the creator and can be used by filters, formatters and sinks.In order to properly use attributes in your code, you need to know its name and type (or at least a set of types it may have).As discussed in the previous sections, attributes can be classified into 3 scopes: global, thread-specific, and source-specific. When a log record is made, attribute values from these 3 sets are joined into a single set and passed to sinks. This implies that the origin of the attribute makes no difference for sinks!Any attribute can be registered in any scope. An attribute is given a unique name upon registration in order to make it possible to search for it. As mentioned earlier, conflicts resolution is based on priority.Attribute registrationThere are common attributes that are likely to be used in nearly any application.Here we take the log record counter attribute and the timestamp attribute for example. Such commonly used attributes can be registered with a single function call:logging::add_common_attributes();Common attributes like “LineID”, “Timestamp”, “ProcessID”, and “ThreadID” are registered globally.Some special attributes are registered automatically for you on logger construction.E.g. the severity_logger registers a source-specific attribute “Severity” which can be used to add a level of emphasis for different log records.The BOOST_LOG_SEV macro acts pretty much like BOOST_LOG except that it takes an additional argument for the open_record method of the logger.Usually we want to register a named_scope attribute so that we can store scope names in log for every log record.E.g.// some where in logger initcore-&gt;add_global_attribute(\"Scope\", attrs::named_scope());...// some where in usagevoid named_scope_logging(){    BOOST_LOG_NAMED_SCOPE(\"named_scope_logging\");    src::severity_logger&lt; severity_level &gt; slg;    BOOST_LOG_SEV(slg, normal) &lt;&lt; \"Hello from the function named_scope_logging!\";}Another useful attribute for performance analysis is what we call the “Timeline” attribute.After registered the “Timeline” attribute e.g. through BOOST_LOG_SCOPED_THREAD_ATTR(\"Timeline\", attrs::timer());, every log record after this will contain the “Timeline” attribute with a high precision time duration passed since the attribute was registered.Based on these readings, one will be able to detect which parts of the code require more or less time to execute. This attribute will be unregistered upon leaving the scope that defined it.Log record formattingAfter adding/registering attributes, you need to specify a formatter that will use these attribute values in order to have them reach the output.As we have seen in the tutorial about writing logs to files, we defined a formatter that used the “TimeStamp” and “Message” attributes.void init(){    logging::add_file_log    (        keywords::file_name = \"sample_%N.log\",        keywords::rotation_size = 10 * 1024 * 1024,        keywords::time_based_rotation = sinks::file::rotation_at_time_point(0, 0, 0),        keywords::format = \"[%TimeStamp%]: %Message%\"    );    logging::core::get()-&gt;set_filter    (        logging::trivial::severity &gt;= logging::trivial::info    );}The format parameter allows us to specify the format of the log record. If you want to set up sinks manually, you can achieve this by using the set_formatter member function of the sink frontends.Lambda-style formattersE.g.keywords::format =(    expr::stream        &lt;&lt; expr::attr&lt; unsigned int &gt;(\"LineID\")        &lt;&lt; \": &lt;\" &lt;&lt; logging::trivial::severity        &lt;&lt; \"&gt; \" &lt;&lt; expr::smessage)The stream is a placeholder for the stream to format the record in. The insertion arguments like attr and message are manipulators that define what should be stored in the stream. Note that it is possible to replace severity with the following expr::attr&lt; logging::trivial::severity_level &gt;(\"Severity\").It is recommended to define placeholders like severity for user’s attributes since it provides simpler syntax in the template expressions and makes coding less error-prone.Boost.Format-style formattersE.g.sink-&gt;set_formatter(    expr::format(\"%1%: &lt;%2%&gt; %3%\")        % expr::attr&lt; unsigned int &gt;(\"LineID\")        % logging::trivial::severity        % expr::smessage);The format placeholder accepts the format string like printf with positional specs of all arguments being formatted.Note that only positional format is currently supported, which means you cannot have all features in the Boost.Format.When you call format(s) where s is the format-string, it constructs an obj that parses the format string and looks for all directives in it and prepares internal structures for the next step.Then you feed variables into the formatter.Once all args have been fed you can dump the format obj to a stream.All in all, the format class translates a format-string into operations on an internal stream, and finally returns the result of the formatting as a string or directly into an output stream.Specialized formattersThese specialized formatters are designed for a number of special types like date, time, named scope. They provide extended control over the formatted values.E.g. You can describe date and time format with a format string compatible with Boost.DateTimeexpr::stream    &lt;&lt; expr::format_date_time&lt; boost::posix_time::ptime &gt;(\"TimeStamp\", \"%Y-%m-%d %H:%M:%S\")    &lt;&lt; \": &lt;\" &lt;&lt; logging::trivial::severity    &lt;&lt; \"&gt; \" &lt;&lt; expr::smessageString templates as formattersIn some contexts, textual templates are accepted as formatters and the lib init support code is invoked to parse the template and reconstruct the appropriate formatter.It’s suitable for simple formatting needs but keep in mind when using this approach there are a number of caveats and you should be careful if you want to use it in a complex formatting need.E.g.keywords::format = \"[%TimeStamp%]: %Message%\"The format now accepts a format string template that contain a number of placeholders enclosed with percent signs. Each placeholder contain an attribute value name to insert instead of the placeholder.Note that such format templates are not accepted by sink backends in the set_formatter method! You need to call parse_formatter to parse textual template into a formatter function!Custom formatting functionsYou can add a custom formatter to a sink backend that supports formatting. The formatter is actually a function obj that supports the following signature:// CharT is the target character typevoid (logging::record_view const&amp; rec, logging::basic_formatting_ostream&lt; CharT &gt;&amp; strm);The formatter will be invoked whenever a log record view rec passes filtering and is to be stored in the log.I tried to customize the ThreadID attribute with the lambda style formatter but got no luck, FYI here’s a pretty old discussion about formatting ThreadID.            https://www.boost.org/doc/libs/1_63_0/libs/log/doc/html/index.html &#8617;              https://www.boost.org/doc/libs/1_63_0/libs/log/doc/html/log/defs.html &#8617;              https://www.boost.org/doc/libs/1_63_0/libs/log/doc/html/log/design.html &#8617;              https://www.boost.org/doc/libs/1_63_0/libs/log/doc/html/log/tutorial.html &#8617;      ",
        "url": "/primer/2020/07/12/boost-log-primer/"
      }
      ,
    
      "primer-2020-06-10-googletest-primer": {
        "title": "GoogleTest Primer",
        "content": "GoogleTest 1 Primer (gtest)What is GoogleTestGoogletest is a testing framework to help write better C++ code  supports any kind of tests, not just unit tests  based on the popular xUnit (e.g. JUnit, PyUnit) architectureWhat defines a “good” testA good test should:  Be independent and repeatable  Be well-organized and reflect the structure of the tested code          =&gt; gtest groups related tests into test suits that can share data and subroutines        Be portable and reusable =&gt; platform neutral, compiler neutral  Verbose diagnostic information when test fail          =&gt; gtest does not stop at the first test failure but stops the current test and continues with the rest tests if any.                  =&gt; enable users to detect and fix multiple bugs in a single run-edit-compile cycle                      Liberate test writers from housekeeping chores and focus on the test content          =&gt; gtest automatically keeps track of all tests defined and does not require users to enumerate them in order to run them        Be fast          =&gt; gtest enable users to reuse shared resources across tests and pay for the setup/tear-down only once, without making tests depend on each other      The nomenclature  Test: gtest’s term for “Test Case” in ISTQB 2. Exercise a particular program path with specific input values and verify the results  Test Suit: a group of related tests  Test Program: contains one or more test suitsBasic ConceptsThe usage of GoogleTest starts with writing assertions.  An assertion is a statement that checks whether a condition is true or false.  Assertion result can be success, nonfatal failure, fatal failure.  A fatal failure aborts the current functionTests use assertions to verify the tested code’s behavior. If a test crashes or has a failed assertion, then this test fails.A Test Suit contains one or many tests. It’s usually a group of tests that reflect the structure of the tested code. Tests within a test suite can share common resources (objects, subroutines) by putting them into a test fixture classAssertions  GTest assertions are macros that resemble function calls.  When assertion fails, gtest prints the assertion’s source file, line number, along with a failure message (user can supply a custom failure message which will be appended to gtest’s msg)  Two types of assertions that differ in their effects on the code being tested          ASSERT_* generate fatal failures when they fail and abort the current function. The abortion may skip clean-up code that comes after it and hence may cause a space leak! (particularly if you will also use a heap checker)      EXPECT_* generate non-fatal failures, which don’t abort the current function and allow more than one failure to be reported in a test        Syntax      ASSERT_EQ(x.size(), y.size()) &lt;&lt; \"Vectors x and y are of unequal length\"; // Anything that can be streamed to an ostream can be streamed to an assertion macro      for (int i = 0; i &lt; x.size(); ++i) {    EXPECT_EQ(x[i], y[i]) &lt;&lt; \"Vectors x and y differ at index \" &lt;&lt; i;  }        Basic Assertions                            Fatal Assertions          Nonfatal Assertions          Explaination                                      ASSERT_TRUE(cond)          EXPECT_TRUE(cond)          cond is true                          ASSERT_FALSE(cond)          EXPECT_FALSE(cond)          cond is false                      Binary Comparisons          N.B. value arguments (val1, val2) must be comparable by the assertion’s comparison operator (==, &lt;, etc.) or you’ll get a compilation error.      ASSERT_EQ(val1, val2), ASSERT_NE(val1, val2), etc.      EXPECT_EQ(val1, val2), EXPECT_NE(val1, val2), etc.      You may need to use ASSERT_TRUE or EXPECT_TRUE to assert the equality of two objects of an user-defined type based on the Google C++ Style Guide                  but when possible, ASSERT_EQ is preferred to ASSERT_EQ(val1 == val2) since it tells you val1 and val2’s values on failure                      String Comparisons          use ASSERT_STREQ() rather than ASSERT_EQ      to assert that a C string is NULL, use ASSERT_STREQ(c_str, NULL) or ASSERT_EQ(c_str, nullptr) if c++11 is supported        Pointer Comparisons          use *_EQ(ptr, nullptr) and *_NE(ptr, nullptr) instead of *_EQ(ptr, NULL). (because nullptr is typed)        Floating point Comparisons          use the floating point variations of some of these macros in order to avoid rounding problems (see here for more information)      Simple Tests  Use the TEST() macro to define and name a test function          test functions are ordinary C++ functions that don’t return a value        Use various gtest assertions we discussed above in the test function, along with any other valid C++ statements you want to use  Test result is determined by the assertions.          if any assertion in the test fails (either fatally or non-fatally), or if the test crashes, the entire test fails        Arguments          first argument is the test suite name      second argument is the test’s name within the test suite      both should be valid C++ identifiers and should not contain any underscores        Test’s full name consists of its containing test suite and its individual test name  Tests from different test suites can have the same individual name  Test results are grouped by test suites, and logically related tests should be in the same test suite.      Naming convention for test suits and tests should follow the same convention for naming functions and classes    e.g.     TEST(TestSuiteName, TestName) {   ... test body ... }  Test FixturesEnable users to use the same data configuration for multiple tests.Steps to create a fixture  Derive a class from ::testing::Test and start its body with protected since we want to access the fixture members from sub-classes  Inside the class, declare any objects you plan to use  If necessary, write a default constructor or SetUp() function to prepare the objects for each test. Use override in C++11 to make sure you spelled SetUp correctly!  If necessary, write a destructor or TearDown() function to release any resources you allocated in SetUp().  If needed, define subroutines for your tests to shareHow to use a fixture  use TEST_F() instead of TEST() as it allows you to access objects and subroutines in the test fixture  the first argument defines the test suite name, and it must be the name of the test fixture class  you must first define the test fixture class before you can actually use it in your tests! or you will get the compiler error virtual outside class declaration  gtest will create a fresh test fixture at runtime for each test defined with TEST_F(), then          immediately initialize it via SetUp()      run the test, clean up by calling TearDown()      delete the test fixture        Note that different tests in the same suite have different test fixture objects, and gtest always delete a test fixture before it creates the next one          gtest does not reuse the same test fixture for multiple tests      any changes one test makes to the fixture do not affect other tests            Naming convention for fixture classes are: append the Test to the class name. e.g. give it the name FooTest if you want to test class Foo    e.g.     class QueueTest : public ::testing::Test {  protected:   void SetUp() override {      q1_.Enqueue(1);      q2_.Enqueue(2);      q2_.Enqueue(3);   }       // void TearDown() override {}       Queue&lt;int&gt; q0_;   Queue&lt;int&gt; q1_;   Queue&lt;int&gt; q2_; };     TEST_F(QueueTest, IsEmptyInitially) {   EXPECT_EQ(q0_.size(), 0); }     TEST_F(QueueTest, DequeueWorks) {   int* n = q0_.Dequeue();   EXPECT_EQ(n, nullptr);       n = q1_.Dequeue();   ASSERT_NE(n, nullptr);   EXPECT_EQ(*n, 1);   EXPECT_EQ(q1_.size(), 0);   delete n;       n = q2_.Dequeue();   ASSERT_NE(n, nullptr);   EXPECT_EQ(*n, 2);   EXPECT_EQ(q2_.size(), 1);   delete n; }  Invoking the tests  TEST() and TEST_F() implicitly register their tests with gtest  You can run them with RUN_ALL_TESTS(), which returns 0 if all the tests are successful or 1 otherwise.  RUN_ALL_TESTS() runs all tests in your link unit and they can be from different test suites, or even different source files  You should call RUN_ALL_TESTS() only once! Calling it more than once conflicts with some advanced gtest features and thus is not supportedBehind the scenes, the RUN_ALL_TESTS() macro:  Saves the state of all gtest flags  Creates a test fixture object for the first test  Initializes it via SetUp()  Runs the test on the fixture object  Cleans up the fixture via TearDown()  Deletes the fixture  Restores the state of all gtest flags  Repeats the above steps for the next test, until all tests have runIf a fatal failure happens, the subsequent steps will be skipped!Writing the main() functionMost users should not need to write their own main function and instead they should link with gtest_main(), which defines a suitable entry point.When you want to do something before the tests that cannot be expressed within the framework of fixtures and test suits, then you can consider writing your own main function.If you write your own main function, it should return the value of RUN_ALL_TESTS()!See the starter code here.Advanced gtest topicsExplicit Success and Failure macrosSUCCEED(), FAIL() do not actually test a value or expression and they generate a success or failure directly. They supports streaming of custom messages into them.  SUCCEED()          it does not make the overall test succeed and a test is considered successful only if none of its assertions fail during its execution      it is purely documentary and currently does not generate any user-visible output        FAIL(), ADD_FAILURE(), and ADD_FAILURE_AT()          FAIL() generates a fatal failure      ADD_FAILURE() and ADD_FAILURE_AT() generate nonfatal failure      they are useful when the control flow (rather than the boolean expression) determines the test’s sucecss or failure      FAIL() can only be used in functions that return void        e.g.      switch(expression) {    case 1:       ... some checks ...    case 2:       ... some other checks ...    default:       FAIL() &lt;&lt; \"We shouldn't get here.\";  }  Exception AssertionsThese assertions are for verifying that a piece of code throws or does not throw an exception of the given type.            Fatal      Nonfatal      Explaination                  ASSERT_THROW(statement, exception_type)      EXPECT_THORW(statement, exception_type)      statement throws an exception of the given type              ASSERT_ANY_THROW(statement)      EXPECT_ANY_THROW(statement)      statement throws an exception of any type              ASSERT_NO_THROW(statement)      EXPECT_NO_THROW(statement)      statement does not throw any exception      Value-Parameterized TestsEnable users to test code with different parameters without writing multiple copies of the same test.HowTo  define a fixture class that derived from both testing::Test and testing::WithParamInterface&lt;T&gt; where T is the type of your parameter values. For convenience, you can just derive the fixture class from testing::TestWithParam&lt;T&gt;, which itself is derived from both the required classes. T can be any copyable type and if it is a raw pointer, you are responsible for managing the lifespan of the pointed values.  If your test fixture defines SetUpTestSuite() or TearDownTestSuite() they must be declared public rather than protected in order to use TEST_P.  Use the TEST_P macro to define as many test patterns using this fixture as you want  Inside the test, you can access the parameter by calling GetParam() method of the TestWithParam&lt;T&gt; class  You can then instantiate the test suite with any set of parameters using INSTANTIATE_TEST_SUITE_P()      You must place the INSTANTIATE_TEST_SUITE_P() statement at global or namespace scope, rather than function scope.    e.g.     INSTANTIATE_TEST_SUITE_P(InstantiationName,                          FooTest,                          testing::Values(\"meeny\", \"miny\", \"moe\"));              https://github.com/google/googletest/blob/master/googletest/docs/primer.md &#8617;              https://glossary.istqb.org/en/search/test%20case &#8617;      ",
        "url": "/primer/2020/06/10/googletest-primer/"
      }
      ,
    
      "projects-2020-05-23-talkui": {
        "title": "Talk UI: A Multi-Modal, Conversational Interface for UI Prototyping",
        "content": "AuthorsHongyi Zhang, Mengxin Cao, Ron ChewDemoIntroductionUI designers have long been sketching to realize the interfaces they have in mind for prototyping. However, there is still much that can be done to make designers’ life easier. Current UI design tools are time-consuming and repetitive: drawing sets of graphics and widgets, then specifying their interactions and behaviors often requires a large effort in code and configuration for small results. The iteration process cannot catch up with the speed the designer’s minds are operating at.In this project, we propose “Talk UI”, a rapid multi-modal prototyping tool that takes advantage of natural language to facilitate the design process and accelerate prototype iteration. With Talk UI, designers will be able to create graphical interfaces as well as specify interactive outcomes through talking and demonstration. No more sketching is needed, and the UI design process will be as natural as having a conversation.FeaturesOur Talk UI leverages natural language and voice input from users and supports the following features that we would like to highlight:  Continuous listening of user voice input without necessity of the user specifying start and stop of a conversation  Instantiation of graphical objects and widgets through natural language  Attaching interactive behaviors to existing graphical objects through demonstration  Support of direct manipulation to update graphical object properties through property sheet  Display of voice command history and conversation feedbacks  Static export of the final interface",
        "url": "/projects/2020/05/23/talkui/"
      }
      ,
    
      "papers-2020-05-02-yarn": {
        "title": "Apache Hadoop YARN: Yet Another Resource Negotiator",
        "content": "Paper LinkyarnAuthorsVinod Kumar Vavilapalli, Arun C Murthy, Chris Douglas, Sharad Agarwal, Mahadev Konar, Robert Evans, Thomas Graves, Jason Lowe, Hitesh Shah, Siddharth Seth, Bikas Saha, Carlo Curino, Owen O’Malley, Sanjay Radia, Benjamin Reed, Eric BaldeschwielerMindMapClick here to see the mind map generated via MubuNotesLink to a notes",
        "url": "/papers/2020/05/02/yarn/"
      }
      ,
    
      "papers-2020-05-02-vmware-drs": {
        "title": "VMware Distributed Resource Management: Design, Implementation, and Lessons Learned",
        "content": "Paper Linkdrs-vmtj-mar12AuthorsAjay Gulati, Anne Holler, Minwen Ji, Ganesha Shanmuganathan, Carl Waldspurger, Xiaoyun ZhuMindMapClick here to see the mind map generated via MubuNotesLink to a notes",
        "url": "/papers/2020/05/02/vmware-drs/"
      }
      ,
    
      "papers-2020-05-02-needle-in-a-haystack-efficient-storage-of-billions-of-photos": {
        "title": "Needle in a haystack: efficient storage of billions of photos",
        "content": "Paper LinkhaystackAuthorsDoug Beaver, Sanjeev Kumar, Harry C. Li, Jason Sobel, Peter VajgelMindMapClick here to see the mind map generated via MubuNotesLink to a notes",
        "url": "/papers/2020/05/02/needle-in-a-haystack-efficient-storage-of-billions-of-photos/"
      }
      ,
    
      "papers-2020-05-02-mesos": {
        "title": "Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center",
        "content": "Paper LinkmesosAuthorsBenjamin Hindman, Andy Konwinski, Matei Zaharia, Ali Ghodsi, Anthony D. Joseph, Randy Katz, Scott Shenker, Ion StoicaMindMapClick here to see the mind map generated via MubuNotesLink to a notes",
        "url": "/papers/2020/05/02/mesos/"
      }
      ,
    
      "papers-2020-05-02-borg-omega-kubernetes": {
        "title": "Borg, Omega, and Kubernetes",
        "content": "Paper Link2898442.2898444AuthorsBrendan Burns, Brian Grant, David Oppenheimer, Eric Brewer, and John WilkesMindMapClick here to see the mind map generated via MubuNotesLink to a notes",
        "url": "/papers/2020/05/02/borg-omega-kubernetes/"
      }
      ,
    
      "papers-2020-05-02-b4-sdn": {
        "title": "B4: Experience with a Globally-Deployed Software Defined WAN",
        "content": "Paper Linkb4-sigcomm13AuthorsSushant Jain, Alok Kumar, Subhasree Mandal, Joon Ong, Leon Poutievski, Arjun Singh, Subbaiah Venkata, Jim Wanderer, Junlan Zhou, Min Zhu, Jonathan Zolla, Urs Hölzle, Stephen Stuart and Amin VahdatMindMapClick here to see the mind map generated via MubuNotesLink to a notes",
        "url": "/papers/2020/05/02/b4-sdn/"
      }
      ,
    
      "papers-2020-01-14-dynamiclly-scaling-apps-in-the-cloud": {
        "title": "Dynamiclly Scaling Applications in the Cloud",
        "content": "Paper Link10.1145/1925861AuthorsLuis M. Vaquero, Luis Rodero-Merino, Rajkumar BuyyaSummaryScalability of a cloud service refers to the capability to dynamically grow or shrink (scale up or scale down) the underlying infrastructure based on some metrics (e.g. the number of requests per second). It strongly affects the Quality of Service (QoS) of an application. Cloud computing features scalability and that “makes it different to so-called ‘advanced outsourcing’ solution”.The real scenario is not that ideal however and what we perceived automated scaling for cloud applications is not that perfect yet and some important pending issues still exist. This paper investigated the most notable initiatives concerning “whole application scalability” in the cloud. The authors provide state of the art efforts to this issue and the underlying trends they follow.We mentioned “whole application scalability”, it includes different elements when different abstraction levels applied.For IaaS (Infrastructure as a Service), where cloud service providers offer hardware infrastructures such as virtual machines (VMs) and networks, “whole application scalability” includes:      Sever scalability        Load balancers and load balancing algorithms        Network scalability (application should be able to request bandwidth-provisioned network pipes and other network resources to interconnect them, the so-called NaaS)  To be more clear, we can divide IaaS into horizontal scaling and vertical scaling.      Horizontal scaling: add new server replicas and load balancers to distribute load among all available replicas. This is where we have more factors to consider, e.g. load balancing algorithms, load balancing load balancers, network slicing, dynamic bandwidth allocation, etc.        Vertical scaling: on-the-fly changing of the assigned resources to an already running instance. Common OSs usually do not support on-the-fly changes on the existing hardware, and sometimes a new VM is provisioned to seamlessly replace the old VM.  For PaaS (Platform as a Service), where the cloud service providers (CSPs) offer a container-like environment and developers deploy their applications as software components, “whole application scalability” here we examine container replication and database replication (there are other factors as well).  Container Scalability                  At container level, there are two ways to run user’s components.                              Run them on non-shared containers, which limits the level of scalability.                                Share containers across users (i.e. enabling multitenant containers), which usually gives better scalability but requires strong isolation among users.                          Both approaches implement scalability by replicate the containers so that user components can run (similar to horizontal scaling). This should be done automatically by the platform, which implies developers should be aware of that when designing software components (e.g. stateless v.s. stateful components).              Database Scalability                  At the database level, there are many discussions on DBMS scalability in literature. Here we focus on 3 mechanisms:                              Distributed caching                                NoSQL databases                                Database clustering                                      Database replication is the most important issue to consider when scaling a PaaS platform (major problem comes from the fact that transactions require protecting the data involved while the transaction lasts, often making that data unavailable to other transactions).            ",
        "url": "/papers/2020/01/14/dynamiclly-scaling-apps-in-the-cloud/"
      }
      ,
    
      "papers-2019-12-13-distributed-graphlab": {
        "title": "Distributed GraphLab: A Framework for Machine Learning in the Cloud",
        "content": "Paper Link1204.6078v1AuthorsYucheng Low, Joseph Gonzalez, Aapo Kyrola, Danny Bickson, Carlos Guestrin, Joseph M. HellersteinMindMapClick here to see the mind map generated via MubuNotesLink to a notes",
        "url": "/papers/2019/12/13/distributed-graphlab/"
      }
      ,
    
      "papers-2019-11-17-multimodal-ml-survey": {
        "title": "Multimodal Machine Learning: A survey and Taxonomy",
        "content": "Paper Link1705.09406AuthorsTadas Baltrušaitis, Chaitanya Ahuja, Louis-Philippe MorencySummary  Multimodal machine learning is about integrating different information sources (called modalities) and build a more performant and robust model. The authors did a comprehensive survey on recent progress in multimodal ML and unified these advances under the same taxonomy.  The authors did a historical review on the applications of multimodal ML such as audio-visual speech recognition and image captioning, they categorized the applications and use that to motivate their discussion on the common technical challenges faced by these applications. This paper summarized 5 major challenges including representation, translation, alignment, fusion, and co-learning.  This paper provides an in-depth and comprehensive survey on the above-summarized challenges faced by the Multimodal ML field. For each challenge, the authors reviewed previous attempts in approaching them and formally defined a common taxonomy for each challenge. Representation is defined over Joint or Coordinated representation, Translation is unified into example-based and generative approaches, Alignment can be divided into explicit and implicit, Fusion could be model-agnostic or model-based, Co-learning could be parallel, non-parallel or hybrid based on the resources available to that modality.",
        "url": "/papers/2019/11/17/multimodal-ml-survey/"
      }
      ,
    
      "papers-2019-09-29-scaling-distributed-ml-through-parameter-server": {
        "title": "Scaling Distributed ML with the Parameter Server",
        "content": "Paper Linkparameter_server_osdi14.pdfAuthorsMu Li, David G. Andersen, Jun Woo Park, Alexander J. Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J. Shekita, Bor-Yiing SuSummary  Proposed a new general purpose framework called Parameter Server for distributed machine learning  Partitioned cluster nodes into server group and worker group and use scheduler to coordinate their communication  Used  various consistency model and asynchronous communication to provide flexibility, scalability and fault tolerance  The proposed framework is general purpose, which  means it can be applied to arbitrary ML algorithms  Considered the characteristics of ML parameters (i.e. vectors, and linear algebra operations between matrices) in their framework and thus enable efficient communication and reduced bandwidth  Since the cluster nodes are usually commodity hardware, they used an optimized data replication architecture to replicate data across server nodes to provide fault tolerance and durabilityNotes about scalable Machine Learning1  What opportunities Parameter Servers provides?          Users do not need to use complicated distributed programming tools to build distributed ML application                  How?                          Notice that the iterative-convergent nature of ML algorithms                                  Explore relaxed consistency models for controlled async parallel execution of ML programs to improve overall system efficiency                                                                          How to deal with Big data in ML computation?          This paper’s approach: Push/Poll gradients between server nodes and worker nodes    - arguably less convenient since users should explicitly decide which parts to be communicated    - more aggressive efficiency gain                  Other ways: Distributed Shared Memory (DSM) that allow programmers to treat the entire cluster as a single memory pool and each node can read/write to any model parameter via programming interface                          Benefits: facilitates implementation without worrying low-level communication                                            Scalable ML categories          General-purpose, programmable libs or frameworks    - user-programmable    - extensible to handle arbitrary ML applications    - e.g. GraphLab, Parameter Server                  Special purpose solvers for specific ML applications                          non-programmable              restricted to predefined ML applications                                  e.g.                                          CCD++ for Matrix Factorization;                      Vowpal Wabbit for regression/classification via Stochastic optimization;                                                  Yahoo LDA and Google plda for topic modeling                                                                                                                                                              Compare to Hadoop/Spark          Hadoop or Spark do not have superior ML algorithm performance compared to proposed frameworks like Parameter Server, GraphLab      Hadoop or Spark do not support flexible consistency model and enforce strict consistency; But they do ensure program portability, reliability and fault tolerance                  Analysis of High-Performance Distributed ML at Scale through Parameter Server Consistency Models &#8617;      ",
        "url": "/papers/2019/09/29/scaling-distributed-ml-through-parameter-server/"
      }
      
    
  };
</script>

<script src="/public/js/lunr.min.js"></script>

<script src="/public/js/search.js"></script>


    </div>

  </body>
</html>
