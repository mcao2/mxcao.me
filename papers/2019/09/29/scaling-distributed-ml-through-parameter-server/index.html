<!DOCTYPE html>
<html lang="en-us">

  <head>
  <meta charset="utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- Meta -->
  <link rel="canonical" href="https://mxcao.me/">

  <!-- Analytics -->
  <script async src="https://umami.mxcao.me/script.js" data-website-id="4c2d93d9-a90e-41c6-bc74-d2f96994b1e9"></script>

  <!-- Christmax Snow -->
  <!-- <script src="https://app.embed.im/snow.js" defer></script> -->

  <title>
    Scaling Distributed ML with the Parameter Server &middot; mxin
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/main.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet"
    href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700%7CSource+Sans+Pro:400,400i,700,700i&subset=greek,greek-ext,latin-ext">

  <!-- Icons -->
  <link rel="apple-touch-icon" sizes="180x180" href="/public/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/public/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/public/favicon-16x16.png">
  <link rel="manifest" href="/public/site.webmanifest">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

</head>

  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>mxin</h1>
      <p class="tagline"># Mengxin Cao</p>
    </div>

    <input type="checkbox" id="menu-icon">
    <label class="menu-label" for="menu-icon"></label>
    <div class="trigger">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-item">
          <a href="/">Home</a>
        </li>
        

        
        
        
        
        
        
        
        
        <li class="sidebar-nav-item">
          <a href="/about/">About</a>
        </li>
        
        
        
        
        
        
        
        
        
        
        
        <li class="sidebar-nav-item">
          <a href="/posts/">Posts</a>
        </li>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <li class="sidebar-nav-item">
          <a href="https://github.com/mcao2/">GitHub</a>
        </li>
        <li class="sidebar-nav-item">
          <a href="https://www.linkedin.com/in/mxin/">LinkedIn</a>
        </li>
        <!-- <li class="sidebar-nav-item">
          <a href="https://cs.cmu.edu/~mcao2/mengxin_cv.pdf">Curriculum Vitae</a>
        </li> -->
      </ul>

      <p class="copyright">&copy; 2023. All rights reserved.</p>
    </div>
  </div>
</div>

    <div class="content container">
      <div class="post">
  <h1 class="post-title">Scaling Distributed ML with the Parameter Server</h1>
  <span class="post-info">29 Sep 2019</span>
  <h2 id="paper-link">Paper Link</h2>

<p><a href="https://www.cs.cmu.edu/~muli/file/parameter_server_osdi14.pdf">parameter_server_osdi14.pdf</a></p>

<h2 id="authors">Authors</h2>

<p><a href="https://www.cs.cmu.edu/~muli/">Mu Li</a>, David G. Andersen, Jun Woo Park, Alexander J. Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J. Shekita, Bor-Yiing Su</p>

<!--more-->

<h2 id="summary">Summary</h2>

<ul>
  <li>Proposed a new general purpose framework called Parameter Server for distributed machine learning</li>
  <li>Partitioned cluster nodes into server group and worker group and use scheduler to coordinate their communication</li>
  <li>Used  various consistency model and asynchronous communication to provide flexibility, scalability and fault tolerance</li>
  <li>The proposed framework is general purpose, which  means it can be applied to arbitrary ML algorithms</li>
  <li>Considered the characteristics of ML parameters (i.e. vectors, and linear algebra operations between matrices) in their framework and thus enable efficient communication and reduced bandwidth</li>
  <li>Since the cluster nodes are usually commodity hardware, they used an optimized data replication architecture to replicate data across server nodes to provide fault tolerance and durability</li>
</ul>

<h2 id="notes-about-scalable-machine-learning">Notes about scalable Machine Learning<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></h2>

<ul>
  <li>What opportunities Parameter Servers provides?
    <ul>
      <li>Users do not need to use complicated distributed programming tools to build distributed ML application
        <ul>
          <li>How?
            <ul>
              <li>Notice that the iterative-convergent nature of ML algorithms
                <ul>
                  <li>Explore relaxed consistency models for controlled async parallel execution of ML programs to improve overall system efficiency</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>How to deal with Big data in ML computation?
    <ul>
      <li>This paper’s approach: <strong>Push/Poll</strong> gradients between server nodes and worker nodes
    - arguably less convenient since users should explicitly decide which parts to be communicated
    - more aggressive efficiency gain
        <ul>
          <li>Other ways: <strong>Distributed Shared Memory</strong> (DSM) that allow programmers to treat the entire cluster as a single memory pool and each node can read/write to any model parameter via programming interface
            <ul>
              <li>Benefits: facilitates implementation without worrying low-level communication</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Scalable ML</strong> categories
    <ul>
      <li>General-purpose, programmable libs or frameworks
    - user-programmable
    - extensible to handle arbitrary ML applications
    - e.g. GraphLab, Parameter Server
        <ul>
          <li>Special purpose solvers for specific ML applications
            <ul>
              <li>non-programmable</li>
              <li>restricted to predefined ML applications
                <ul>
                  <li>e.g.
                    <ul>
                      <li>CCD++ for Matrix Factorization;</li>
                      <li>Vowpal Wabbit for regression/classification via Stochastic optimization;
                        <ul>
                          <li>Yahoo LDA and Google plda for topic modeling</li>
                        </ul>
                      </li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Compare to Hadoop/Spark
    <ul>
      <li>Hadoop or Spark do not have superior ML algorithm performance compared to proposed frameworks like Parameter Server, GraphLab</li>
      <li>Hadoop or Spark do not support flexible consistency model and enforce strict consistency; But they do ensure program portability, reliability and fault tolerance</li>
    </ul>
  </li>
</ul>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><a href="https://petuum.com/wp-content/uploads/2019/01/Analysis_High-Performance_Distributed_ML_Scale.pdf">Analysis of High-Performance Distributed ML at Scale through Parameter Server Consistency Models</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
    <li>
      <h3>
        <a href="/2023/06/22/how-to-setup-ptr-record-in-oci/">
          How to setup PTR record in Oracle Cloud Infrastructure (OCI)
          <small>22 Jun 2023</small>
        </a>
      </h3>
    </li>
    
    <li>
      <h3>
        <a href="/2023/06/21/Debug-resource-deadlock-avoided/">
          Debug Resource Deadlock Avoided Error
          <small>21 Jun 2023</small>
        </a>
      </h3>
    </li>
    
    <li>
      <h3>
        <a href="/tools/2022/08/28/setup-vmess-edge/">
          Set up network edge router via V2Ray
          <small>28 Aug 2022</small>
        </a>
      </h3>
    </li>
    
  </ul>
</div>
    </div>

  </body>
</html>
